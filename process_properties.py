from pyspark.sql import SparkSession
from pyspark.sql.types import StringType, StructType, StructField, FloatType
import urllib2
import StringIO, zipfile

class file_properties(object):
    def __init__(self):
        pass

    def _get_compressed_size(self, file, bytes_flag):

        file_ext = '.asm'
        if bytes_flag:
            file_ext = '.bytes'

        zipped_file = StringIO.StringIO()
        zipped_file_object = zipfile.ZipFile(zipped_file, 'w', zipfile.ZIP_DEFLATED)
                                
        # The file 'file' is witten to the archive zipped_file_here
        zipped_file_object.writestr('file' + file_ext, file.getvalue()) #try writestr with get value
        zipped_file_object.close()

        size = zipped_file_object.getinfo('file' + file_ext).compress_size
        
        return size

    def _get_size_ratios(self, hash_name):

        urls = ['https://s3.amazonaws.com/eds-uga-csci8360/data/project2/metadata/' + hash_name + '.asm', \
        'https://s3.amazonaws.com/eds-uga-csci8360/data/project2/binaries/' + hash_name + '.bytes']
        
        raw_sizes = [] # zeroth position to hold asm file size
        compressed_sizes = []
        bytes_flag = False

        for url in urls:
            response = urllib2.urlopen(url)
            
            info = str(response.info())
                        
            s = info.find('Content-Length:')
            e = info.find('Server')
            
            size = info[(s+15):e].lstrip().rstrip()
            
            if url.endswith('s'):
                raw_sizes.append(float(size))
                bytes_flag = True
            else:
                raw_sizes.append(float(size))

            #debug
            print "raw_sizes"
            print raw_sizes

            # -------------------------------------------------------------------- #
            # Get the size of the compressed file. All processing is done in-memory
            # without the need of disk IO, god bless StringIO
            # -------------------------------------------------------------------- #

            # This file holds the response data
            in_memory_file = StringIO.StringIO(response.read())
            in_memory_file.write(response.read())

            size_compressed = self._get_compressed_size(in_memory_file, bytes_flag)
            
            if bytes_flag:
                compressed_sizes.append(float(size_compressed))
            else:
                compressed_sizes.append(float(size_compressed))

            print "compressed_sizes"
            print compressed_sizes

            # do not delete
            in_memory_file.close()
            response.close()

        # return list of ratios with first postition having ratio of raw file sizes and 
        # second with compressed file sizes
        ab = raw_sizes[0] / raw_sizes[1]
        abc = compressed_sizes[0] / compressed_sizes[1]
        return [hash_name, ab, abc, ab/abc]
        
    def transform(self, x_rdd, y_rdd = None):
        
        x_processed_rdd = x_rdd.map(self._get_size_ratios)

        return x_processed_rdd

if __name__ == '__main__':

    x_train_file = 's3n://eds-uga-csci8360/data/project2/labels/X_train_small.txt'
    x_test_file = 's3n://eds-uga-csci8360/data/project2/labels/X_test_small.txt'

    spark = SparkSession\
            .builder\
            .appName("Test")\
            .getOrCreate()

    sc = spark.sparkContext
    sc.setLogLevel("INFO")

    ***REMOVED***
    ***REMOVED***

    x_train = sc.textFile(x_train_file,20)
    x_test = sc.textFile(x_test_file,20)

    rdds = [x_train, x_test]

    train_file_flag = True

    for rdd in rdds:
        properties = file_properties()

        #debug
        print "transforming now"

        properties_rdd = properties.transform(rdd)
        properties_rdd = properties_rdd.map(lambda r: [r[0], r[1], r[2], r[3]])

        schema = StructType([StructField('hash', StringType(), True), \
            StructField('ab_ratio', FloatType(), True), \
            StructField('abc_ratio', FloatType(), True), \
            StructField('ab2abc_ratio', FloatType(), True)])

        #properties_df = properties_df.map(lambda x: [x])
        properties_df = properties_rdd.toDF(schema)
        #properties_df.show(truncate=False)

        # write to parquet
        try:
            if train_file_flag:
                properties_df.write.save('./data/train_small_properties.parquet')
                train_file_flag = False
            else:
                properties_df.write.save('./data/test_small_properties.parquet')
        except:
            print 'could not write parquet file'
            raise
    spark.stop()