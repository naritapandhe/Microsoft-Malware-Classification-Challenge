from pyspark.sql import SparkSession
from pyspark.ml.classification import RandomForestClassifier
from pyspark.sql.types import *
from pyspark.sql import functions as sql_functions
from pyspark.ml.linalg import SparseVector,Vectors, VectorUDT
import numpy as np
import sys


def calmean(a1,a2,a3,a4,a5,a6,a7,a8,a9,a10):
    a1 = np.array(a1).astype(np.float128)
    a2 = np.array(a2).astype(np.float128)
    a3 = np.array(a3).astype(np.float128)
    a4 = np.array(a4).astype(np.float128)
    a5 = np.array(a5).astype(np.float128)
    a6 = np.array(a6).astype(np.float128)
    a7 = np.array(a7).astype(np.float128)
    a8 = np.array(a8).astype(np.float128)
    a9 = np.array(a9).astype(np.float128)
    a10 = np.array(a10).astype(np.float128)

    t = np.array([a1,a2,a3,a4,a5,a6,a7,a8,a9,a10])
    t1 = np.mean(t, axis=0)
    return np.argmax(t1)

def main():
        
#-------------------------------------------------------------------------------
# Initialize spark session
#-------------------------------------------------------------------------------
    spark = SparkSession\
            .builder\
            .appName("Test")\
            .getOrCreate()
    
    sc = spark.sparkContext

    sc._jsc.hadoopConfiguration().set("fs.s3n.awsAccessKeyId", "***REMOVED***")
    sc._jsc.hadoopConfiguration().set("fs.s3n.awsSecretAccessKey", "***REMOVED***")

#-------------------------------------------------------------------------------
# Read the extra args: This determines whether accuracy is to be calculated,
# because, it is to be done only when actual labels from y_test also exist
#-------------------------------------------------------------------------------

    options = (sys.argv)
    isTestSetPresent = options[1]

#-------------------------------------------------------------------------------
# Read the parquets
#-------------------------------------------------------------------------------
    train_bytes = spark.read.load("./newparquet/train.parquet")
    test_bytes = spark.read.load("./newparquet/test.parquet")
    print "Read all the parquets....."
    
    seeds = {0:131,1:17,2:1341,3:563,4:23,5:9,6:42,7:90,8:222,9:100}
    for i in range(10):
        probabilityColName = str(i) + '_probabilityColName'
        rf = RandomForestClassifier(numTrees=200,maxDepth=8,labelCol="label",featuresCol="features",seed=seeds[i],impurity="gini")
        model = rf.fit(train_bytes)
        print "rf fit done....."

        intermediate_test_bytes = model.transform(test_bytes)
        intermediate_test_bytes =  intermediate_test_bytes.drop('rawPrediction')
        intermediate_test_bytes = intermediate_test_bytes.drop('prediction')
        test_bytes =  intermediate_test_bytes.withColumnRenamed('probability',probabilityColName)
        
    print "rf transform done....."
    print "results done!!"

    #Average is calculated here
    test_bytes =  test_bytes.select('hash','0_probabilityColName','1_probabilityColName','2_probabilityColName','3_probabilityColName','4_probabilityColName','5_probabilityColName','6_probabilityColName','7_probabilityColName','8_probabilityColName','9_probabilityColName','label')
    mean_probs = test_bytes.rdd.map(lambda (hsh,p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,label):(hsh,calmean(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10),int(label)))


    if isTestSetPresent:
        '''
        Accuracy is calculated only when the labels in y_test
        are also provided. Otherwise, the average are just stored
        in a file
        '''
        mean_probs_pred = mean_probs.map(lambda (hsh,x1,x2):(hsh,(x1,x2))) 
        matched = mean_probs_pred.filter(lambda (x,(y1,y2)):y1==y2).count()
        unmatched = mean_probs_pred.filter(lambda (x,(y1,y2)):y1!=y2).count()
        print ("matched",matched)
        print ("unmatched",unmatched)
    else:
        schema = StructType([StructField('hash',StringType(),True),StructField('average_probs',DoubleType(),True),StructField('dummy_label',IntegerType(),True)])   
        mean_probs = mean_probs.toDF(schema)
        mean_probs.write.save("./experiments/average_probs.parquet")
        print "Average Probabilties are created!!"


    spark.stop()
    
    

if __name__ == "__main__":
    main()    

