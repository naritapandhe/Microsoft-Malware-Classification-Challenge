from pyspark.sql import SparkSession
from pyspark.ml.feature import PCA
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.linalg import SparseVector, VectorUDT, Vectors
from pyspark.sql.types import *
from pyspark.ml.feature import VectorAssembler


def main():
        
#-------------------------------------------------------------------------------
# Initialize spark session
#-------------------------------------------------------------------------------
    spark = SparkSession\
            .builder\
            .appName("Test")\
            .getOrCreate()
    
    sc = spark.sparkContext

    sc._jsc.hadoopConfiguration().set("fs.s3n.awsAccessKeyId", "***REMOVED***")
    sc._jsc.hadoopConfiguration().set("fs.s3n.awsSecretAccessKey", "***REMOVED***")

#-------------------------------------------------------------------------------
# Step 1:23
#-------------------------------------------------------------------------------
    train_bytes = spark.read.load("./newparquet/train.parquet")
    test_bytes = spark.read.load("./newparquet/test.parquet")
    print "Read all the parquets....."

    seeds = {0:131,1:17,2:1341,3:563,4:23}
    all_rdds = {}
    for i in range(3):
        print ("i: ",i)
        #rf classifier
        predictionColName = str(i) + '_predictedLabel'
        probabilityColName = str(i) + '_probabilityColName'
        rf = RandomForestClassifier(numTrees=200,maxDepth=8,labelCol="label",featuresCol="features",probabilityCol=probabilityColName,predictionCol=predictionColName,seed=seeds[i])
        model = rf.fit(train_bytes)
        print "rf fit done....."

        result = model.transform(test_bytes)
        all_rdds[i] = result
        fileName = "./rfOps/"+str(i)+"_prediction.csv"
        result.select('hash',probabilityColName,predictionColName).toPandas().to_csv(fileName,header=False,index=False)
        result.write.save(fileName+".parquet")
        print "rf transform done....."
        
    #print all_rdds[0].show()
    #print all_rdds[1].show()

    print "results done!!"


    spark.stop()
    
    

if __name__ == "__main__":
    main()    

