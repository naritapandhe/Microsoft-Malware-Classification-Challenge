from pyspark.sql import SparkSession
from pyspark.ml.feature import PCA
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.linalg import SparseVector, VectorUDT, Vectors
from pyspark.sql.types import *
from pyspark.ml.feature import VectorAssembler
from pyspark.sql.functions import UserDefinedFunction
from pyspark.sql.types import DoubleType
import numpy as np

def calmean(a1,a2,a3):
    a1 = np.array(a1).astype(np.float128)
    a2 = np.array(a2).astype(np.float128)
    a3 = np.array(a3).astype(np.float128)
    t = np.array([a1,a2,a3])
    t1 = np.mean(t, axis=0)

    return np.argmax(t1)
    

def main():
        
#-------------------------------------------------------------------------------
# Initialize spark session
#-------------------------------------------------------------------------------
    spark = SparkSession\
            .builder\
            .appName("Test")\
            .getOrCreate()
    
    sc = spark.sparkContext

    sc._jsc.hadoopConfiguration().set("fs.s3n.awsAccessKeyId", "***REMOVED***")
    sc._jsc.hadoopConfiguration().set("fs.s3n.awsSecretAccessKey", "***REMOVED***")

#-------------------------------------------------------------------------------
# Step 1:23
#-------------------------------------------------------------------------------
    zero_prediction = spark.read.load("./rfOps/0_prediction.csv.parquet")
    first_prediction = spark.read.load("./rfOps/1_prediction.csv.parquet")
    second_prediction = spark.read.load("./rfOps/2_prediction.csv.parquet")


    '''
    Shitty redudant code. Have to fix this, by appending rows
    to existing RDDs when building RFs, so that they can all be
    read at once. 
    Did this, just to see whether any difference is observed.
    '''

    #All the predictions are put together
    zero_prediction_subset = zero_prediction.select('hash','0_probabilityColName','0_predictedLabel','label')
    zero_prediction_subset = zero_prediction_subset.withColumnRenamed('0_probabilityColName','probabilityColName')
    zero_prediction_subset = zero_prediction_subset.withColumnRenamed('0_predictedLabel','predictedLabel')
    zero_prediction_subset = zero_prediction_subset.rdd.map(lambda (hsh,probabilityColName,predictedLabel,label):(hsh,(probabilityColName,predictedLabel,label)))

    first_prediction_subset = first_prediction.select('hash','1_probabilityColName','1_predictedLabel','label')
    first_prediction_subset = first_prediction_subset.withColumnRenamed('1_probabilityColName','probabilityColName')
    first_prediction_subset = first_prediction_subset.withColumnRenamed('1_predictedLabel','predictedLabel')
    first_prediction_subset = first_prediction_subset.rdd.map(lambda (hsh,probabilityColName,predictedLabel,label):(hsh,(probabilityColName,predictedLabel,label)))


    second_prediction_subset = second_prediction.select('hash','2_probabilityColName','2_predictedLabel','label')
    second_prediction_subset = second_prediction_subset.withColumnRenamed('2_probabilityColName','probabilityColName')
    second_prediction_subset = second_prediction_subset.withColumnRenamed('2_predictedLabel','predictedLabel')
    second_prediction_subset = second_prediction_subset.rdd.map(lambda (hsh,probabilityColName,predictedLabel,label):(hsh,(probabilityColName,predictedLabel,label)))
    
   
    print zero_prediction_subset.take(1)
    print "0sssssssth"

    print first_prediction_subset.take(1)
    print "1ssssssth"

    #[(u'9fyAiTPMpZvqX6Khnb7V', ((DenseVector([0.0, 0.005, 0.0, 0.985, 0.0, 0.0, 0.005, 0.0, 0.0, 0.005]), 3.0, 3.0), (DenseVector([0.0, 0.005, 0.0, 0.9712, 0.0005, 0.0, 0.0083, 0.0, 0.005, 0.01]), 3.0, 3.0)))]
    final_prob = zero_prediction_subset.join(first_prediction_subset).map(lambda (hsh,((prob1,plabel1,label1),(prob2,plabel2,label2))):(hsh,(prob1,prob1,label1)))
    
    #[(u'9NxCbYysSItW45mOkXhv', ((DenseVector([0.0, 0.0, 0.0, 0.9847, 0.0003, 0.0, 0.01, 0.0, 0.005, 0.0]), DenseVector([0.0, 0.0, 0.0, 0.9847, 0.0003, 0.0, 0.01, 0.0, 0.005, 0.0]), 3.0), (DenseVector([0.0, 0.0, 0.0, 0.9994, 0.0, 0.0, 0.0, 0.0, 0.0006, 0.0]), 3.0, 3.0)))]
    final_prob =  final_prob.join(second_prediction_subset).map(lambda (hsh,((prob1,prob2,label1),(prob3,plabel,label2))):(hsh,(prob1,prob2,prob3,label2)))
   

    #Average is calculated here
    mean_probs = final_prob.map(lambda (hsh,(prob1,prob2,prob3,label)):(hsh,calmean(prob1,prob2,prob3),int(label)))
    #Since actual label is already present, just compare the predicted against each other
    mean_probs_pred = mean_probs.map(lambda (hsh,x1,x2):(hsh,(x1,x2))) 

    matched = mean_probs_pred.filter(lambda (x,(y1,y2)):y1==y2).count()
    unmatched = mean_probs_pred.filter(lambda (x,(y1,y2)):y1!=y2).count()

    
    print mean_probs_pred.collect()
    print mean_probs_pred.count()
    print ("matched",matched)
    print ("unmatched",unmatched)
    
    spark.stop()
    

if __name__ == "__main__":
    main()    

