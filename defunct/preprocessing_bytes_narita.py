#imports
from pyspark.mllib.util import MLUtils
from pyspark.sql import SparkSession
import os
import shutil as sh
from collections import Counter
from pyspark.rdd import RDD
import numpy as np
from itertools import chain
from pyspark.sql import functions as sql_functions
import sys
from pyspark.sql.types import *
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.linalg import SparseVector,Vectors, VectorUDT
from pyspark.ml.evaluation import MulticlassClassificationEvaluator



class preprocessor_bytes(object):
     
    def __init__(self):
        self._get_most_common_bigrams()
        
    def get_binary_names(self,hsh):
        return 's3n://eds-uga-csci8360/data/project2/binaries/'+hsh+'.bytes'
        #return './subsetData/'+hsh+'.bytes'

    #Reads the most_common_bigrams
    def _get_most_common_bigrams(self):
        bigrams1 = list(np.load('./data/most_common_bigrams.npy'))
        bigrams2 = dict(enumerate(bigrams1))
        self.top_bigrams = dict((y,x) for x,y in bigrams2.iteritems())
        

    def _tokenize(self,docLines):
        docLines = docLines.replace("\r\n"," ")
        tokens = [word for word in docLines.split() if word!='??' and len(word)==2]
        return tokens


    def _extract_common_bigrams(self,allWords):
        common_bigrams = []
        for i in range(0,len(allWords)-1):
            bigram = int(allWords[i]+allWords[i+1],16)
            if bigram in self.top_bigrams:
                common_bigrams.append(self.top_bigrams[bigram])
        
        common_bigrams_dict = dict(Counter(common_bigrams))
        return common_bigrams_dict  

    def hashes_and_labels(self,x_rdd,y_rdd=None,both=True) :
        x_rdd2 = x_rdd.zipWithIndex().map(lambda (hsh,indx):(indx,hsh)).cache()
        if both and y_rdd:
            y_rdd2 = y_rdd.zipWithIndex().map(lambda (lbl,indx):(indx,lbl))
            hash_and_labels = x_rdd2.join(y_rdd2)
        
        else:
            hash_and_labels = x_rdd2

        return x_rdd2, hash_and_labels

    def _tf_idf(self,common_bigrams_frequency_vec):
        '''
        convert row of word token counts into sparse vector of tfidf frequencies
        '''
        sparse_dic = {}
        df_dic = {}
        for index,freq in common_bigrams_frequency_vec.iteritems():
                #sparse_dic[index] = np.log(freq/float(sum(common_bigrams_frequency_vec.itervalues())))    
                sparse_dic[index] = freq
                if index in self.doc_freq:
                    df_dic[index] = self.doc_freq[index]
                else:
                    df_dic[index] = 1
        for key in  sparse_dic:           
            sparse_dic[key] = (1+np.log10(sparse_dic[key]))*(np.log10(float(self.doc_count)/df_dic[key]))                    
        tfidf = SparseVector(len(self.top_bigrams),sparse_dic)
        return tfidf
        
    def transform(self, hash_labels_words,train=True):
        hash_labels_tokenized = hash_labels_words.map(lambda (hsh,lbl,docLines):(hsh,lbl,self._tokenize(docLines)))
        
        #Returns: (hsh,lbl,SparseVector(len(top_common_bigrams),Vector of index of bigrams+counts))
        hash_labels_common_bigrams = hash_labels_tokenized.map(lambda (hsh,lbl,tokens):(hsh,lbl,self._extract_common_bigrams(tokens)))

        if train:
            #populate word count dictionary
            self.doc_freq = hash_labels_common_bigrams.map(lambda (hsh,lbl,common_bigrams): set(common_bigrams)).flatMap(lambda word: word).map(lambda word: (word,1)).reduceByKey(lambda acc, w: acc + w).filter(lambda x: x[1]).collectAsMap()
            self.doc_count = hash_labels_common_bigrams.count()

        #create Tf-Idf vectors
        hash_labels_bigrams_tfidf = hash_labels_common_bigrams.map(lambda (hsh,lbl,common_bigrams):(hsh,lbl,self._tf_idf(common_bigrams)))
        return hash_labels_bigrams_tfidf

    def toDataFrame(self,hash_labels_common_bigrams):
        hash_labels_common_bigrams_df = hash_labels_common_bigrams.map(lambda (hsh,lbl,common_bigrams): (hsh,common_bigrams,float(lbl)))
        schema = StructType([StructField('hash',StringType(),True),StructField('features',VectorUDT(),True),StructField('label',DoubleType(),True)])   
        df = hash_labels_common_bigrams_df.toDF(schema)
        df = df.withColumn('features',df.features.cast(VectorUDT()))
        return df
    

def main():
        
#-------------------------------------------------------------------------------
# Initialize spark session
#-------------------------------------------------------------------------------
    spark = SparkSession\
            .builder\
            .appName("Test")\
            .getOrCreate()
    
    sc = spark.sparkContext

    sc._jsc.hadoopConfiguration().set("fs.s3n.awsAccessKeyId", "AKIAI5AGW2NXUVOJ7L2A")
    sc._jsc.hadoopConfiguration().set("fs.s3n.awsSecretAccessKey", "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")

    xTestFlag = False
    yTestFlag = False
#-------------------------------------------------------------------------------
# Read all the file names
#-------------------------------------------------------------------------------
    fileNames = (sys.argv)
    X_train_file = fileNames[1]
    y_train_file = fileNames[2]

    if(len(fileNames)>=4):
        X_test_file = fileNames[3]
        xTestFlag = True

    if(len(fileNames)==5):
        y_test_file = fileNames[4]
        yTestFlag = True

    
#-------------------------------------------------------------------------------
# Training Data
#-------------------------------------------------------------------------------
    #Get the most common bigrams
    preprocessor1 = preprocessor_bytes()
    
    x_train = sc.textFile(X_train_file)
    y_train = sc.textFile(y_train_file)
   
    #Put hash, labels together. Returns: (indx,hsh) (indx,lbl) => (indx,(hsh,lbl))
    x_train, hash_and_labels = preprocessor1.hashes_and_labels(x_train,y_train,True)
   
    #returns an array of all the training file names
    binary_training_fileNames = x_train.map(lambda (indx,hsh):preprocessor1.get_binary_names(hsh)).collect()
    binary_training_fileNames_broadCast = sc.broadcast(list(binary_training_fileNames))
    training_binaries = sc.wholeTextFiles(','.join(binary_training_fileNames_broadCast.value)).zipWithIndex().map(lambda (words,indx):(indx,words))
    
    #returns: (index,(hash,label),(file_url,words)) => (hash,label,words)
    hash_labels_words = hash_and_labels.join(training_binaries).map(lambda (indx,((hsh,label),(furl,words))):(hsh,label,words))
    
    #tokenizes and extracts common bigrams
    hash_labels_common_bigrams = preprocessor1.transform(hash_labels_words,True)
    #print hash_labels_common_bigrams.take(1)
    
    #4. Convert to dataframe
    trainingDF = preprocessor1.toDataFrame(hash_labels_common_bigrams)
    trainingDF.write.save("./data1/train_small_tfidf_bytes.parquet")
    print "trainingDF created!!"
    print trainingDF.show()
   
#-------------------------------------------------------------------------------
# Testing data
#-------------------------------------------------------------------------------
    if xTestFlag:
        #Get the most common bigrams
        #preprocessor1 = preprocessor_bytes()

        x_test = sc.textFile(X_test_file,36)

        if yTestFlag:
            y_test = sc.textFile(y_test_file,36)
            x_test, test_hash_and_labels = preprocessor1.hashes_and_labels(x_test,y_test,True)

            #returns an array of all the testing file names
            binary_testing_fileNames = x_test.map(lambda (indx,hsh):preprocessor1.get_binary_names(hsh)).collect()
            binary_testing_fileNames_broadCast = sc.broadcast(list(binary_testing_fileNames))
            testing_binaries = sc.wholeTextFiles(','.join(binary_testing_fileNames_broadCast.value),36).zipWithIndex().map(lambda (words,indx):(indx,words))
            test_hash_labels_words = test_hash_and_labels.join(testing_binaries).map(lambda (indx,((hsh,label),(furl,words))):(hsh,label,words))

            
        else:
            y_test=None
            x_test, test_hash_and_labels = preprocessor1.hashes_and_labels(x_test,y_test,False)

            #returns an array of all the testing file names
            binary_testing_fileNames = x_test.map(lambda (indx,hsh):preprocessor1.get_binary_names(hsh)).collect()
            binary_testing_fileNames_broadCast = sc.broadcast(list(binary_testing_fileNames))
            testing_binaries = sc.wholeTextFiles(','.join(binary_testing_fileNames_broadCast.value),36).zipWithIndex().map(lambda (words,indx):(indx,words))
            test_hash_labels_words = test_hash_and_labels.join(testing_binaries).map(lambda (indx,((hsh),(furl,words))):(hsh,1,words))

        
        #tokenizes and extracts common bigrams
        test_hash_labels_common_bigrams = preprocessor1.transform(test_hash_labels_words,False)

        #4. Convert to dataframe
        testingDF = preprocessor1.toDataFrame(test_hash_labels_common_bigrams)
        testingDF.write.save("./data1/test_small_tfidf_bytes.parquet")
        print "testingDF created!!"
        print testingDF.show()

#--------------------------------------------------------------------------------------------
# Random Forests: 
#    - If only X_test is given, then the below code just builds a RF model
#    - If both X_test, y_test are given, only then accuracies of predictions are calculated
#--------------------------------------------------------------------------------------------
    '''
    rf = RandomForestClassifier(numTrees=200,impurity='gini',maxDepth=8, maxBins=32,seed=42)
    model = rf.fit(trainingDF)
    print "model fit!!"

    if yTestFlag:
 
        # Make predictions.
        result = model.transform(testingDF)
        print "model transformed!!"

        #save to csv
        result.select('hash','prediction').toPandas().to_csv('/home/ec2-user/finalLargePredictions.csv',header=False,index=False)
        print "results written to csv"

        #get accuracy
        predictionAndLabels = result.select("prediction", "label")
        evaluator = MulticlassClassificationEvaluator(metricName="accuracy")
        print "Accuracy: " + str(evaluator.evaluate(predictionAndLabels))
        spark.stop()
    '''
    

if __name__ == "__main__":
    main()    

