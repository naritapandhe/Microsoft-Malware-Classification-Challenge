#Step:1
#Input
# X_train: 
# hash1, hash2, hash3
#Y_train:
# L1, L2, L3

#binaries/
#hash1.bytes, hash2.bytes, hash3.bytes

#Expected Output: <hash,label,words>

#Training Data Format:
#X_train, y_train, binaries

#Read most_common_bigrams
#Clean and create bigrams of all the words
#Extract the most common bigrams
#Return <hash,label,bigrams>

#Random Forests works with Vectors, and so
#we need to convert our common_bigrams to a vector!!
#Return <hash,label,SparseVector(len,{indx:count})>


#Testing Data format:
#X_test, binaries
#Final Expected Output: <hash, predictedLabel, words>


from pyspark.mllib.util import MLUtils
from pyspark.sql import SparkSession
import os
import shutil as sh
from collections import Counter
from pyspark.rdd import RDD
import numpy as np
from itertools import chain
from pyspark.sql import functions as sql_functions
import sys
from pyspark.sql.types import *
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.linalg import SparseVector,Vectors, VectorUDT
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

def get_binary_names(hsh):
    return 's3n://eds-uga-csci8360/data/project2/binaries/'+hsh+'.bytes'


#Creates a dictionary of the format: {bigram:index}   
def get_most_common_bigrams():
    bigrams1 = list(np.load('./data/most_common_bigrams.npy'))
    bigrams2 = dict(enumerate(bigrams1))
    top_bigrams = dict((y,x) for x,y in bigrams2.iteritems())
    return top_bigrams

def extract_bigrams(docLines):
    docLines = docLines.replace("\r\n"," ")
    tokens = [word for word in docLines.split() if word!='??' and len(word)==2]
    return tokens


def extract_common_bigrams(allWords,top_common_bigrams):
    common_bigrams = []
    for i in range(0,len(allWords)-2):
        bigram = int(allWords[i]+allWords[i+1],16)
        if bigram in top_common_bigrams:
            common_bigrams.append(top_common_bigrams[bigram])
    
    sparse_dic = dict(Counter(common_bigrams))
    common_bigrams_frequency = SparseVector(len(top_common_bigrams),sparse_dic)
    return common_bigrams_frequency        

def main():
        
    #if len(sys.argv) < 4:
    #    sys.exit('Please specify all the file names in the following order: X_train, y_train, X_test, y_test')

#-------------------------------------------------------------------------------
# Initialize spark session
#-------------------------------------------------------------------------------
    spark = SparkSession\
            .builder\
            .appName("Test")\
            .getOrCreate()
    
    sc = spark.sparkContext

    sc._jsc.hadoopConfiguration().set("fs.s3n.awsAccessKeyId", "***REMOVED***")
    sc._jsc.hadoopConfiguration().set("fs.s3n.awsSecretAccessKey", "***REMOVED***")

#-------------------------------------------------------------------------------
# Step 1:23
#-------------------------------------------------------------------------------
    fileNames = (sys.argv)
    X_train_file = fileNames[1]
    y_train_file = fileNames[2]

    x_train = sc.textFile(X_train_file,24).zipWithIndex().map(lambda (hsh,indx):(indx,hsh)).cache()
    y_train = sc.textFile(y_train_file,24).zipWithIndex().map(lambda (lbl,indx):(indx,lbl))
   
    #Returns: (indx,hsh) (indx,lbl) => (indx,(hsh,lbl))
    hash_and_labels = x_train.join(y_train)
   
    #returns an array of all the training file names
    binary_training_fileNames = x_train.map(lambda (indx,hsh):get_binary_names(hsh)).collect()
    binary_training_fileNames_broadCast = sc.broadcast(list(binary_training_fileNames))
    training_binaries = sc.wholeTextFiles(','.join(binary_training_fileNames_broadCast.value),24).zipWithIndex().map(lambda (words,indx):(indx,words))
   
    #returns: (index,(hash,label),(file_url,words)) => (hash,label,words)
    hash_labels_words = hash_and_labels.join(training_binaries).map(lambda (indx,((hsh,lbl),(furl,words))):(hsh,lbl,words))
    
    #Get the most common bigrams
    top_bigrams = get_most_common_bigrams()
    top_bigrams_broadcast = sc.broadcast(top_bigrams)


    hash_labels_bigrams = hash_labels_words.map(lambda (hsh,lbl,words):(hsh,lbl,extract_bigrams(words)))
    #print hash_labels_bigrams.count()
    
    #Returns: (hsh,lbl,SparseVector(len(top_common_bigrams),Vector of index of bigrams+counts))
    hash_labels_common_bigrams = hash_labels_bigrams.map(lambda (hsh,lbl,bigrams):(hsh,lbl,extract_common_bigrams(bigrams,top_bigrams_broadcast.value)))
    
    #4. Convert to dataframe
    training_hash_labels_common_bigrams_df = hash_labels_common_bigrams.map(lambda (hsh,lbl,common_bigrams): (hsh,common_bigrams,float(lbl)))
    schema = StructType([StructField('hash',StringType(),True),StructField('features',VectorUDT(),True),StructField('label',DoubleType(),True)])   
    trainingDF = training_hash_labels_common_bigrams_df.toDF(schema)
    trainingDF = trainingDF.withColumn('features',trainingDF.features.cast(VectorUDT()))
    print "trainingDF created!!"
    
#-------------------------------------------------------------------------------
# Steps: For testing data
#-------------------------------------------------------------------------------
    X_test_file = fileNames[3]
    #y_test_file = fileNames[4]

    x_test = sc.textFile(X_test_file,24).zipWithIndex().map(lambda (hsh,indx):(indx,hsh)).cache()
    #y_test = sc.textFile(y_test_file).zipWithIndex().map(lambda (lbl,indx):(indx,lbl))
   
    #Returns: (indx,hsh) (indx,lbl) => (indx,(hsh,lbl))
    test_hash_and_labels = x_test

    #returns an array of all the testing file names
    binary_testing_fileNames = x_test.map(lambda (indx,hsh):get_binary_names(hsh)).collect()
    binary_testing_fileNames_broadCast = sc.broadcast(list(binary_testing_fileNames))
    testing_binaries = sc.wholeTextFiles(','.join(binary_testing_fileNames_broadCast.value),24).zipWithIndex().map(lambda (words,indx):(indx,words))

    #returns: (index,(hash,label),(file_url,words)) => (hash,label,words)
    test_hash_labels_words = test_hash_and_labels.join(testing_binaries).map(lambda (indx,((hsh),(furl,words))):(hsh,words))
    test_hash_labels_bigrams = test_hash_labels_words.map(lambda (hsh,words):(hsh,extract_bigrams(words)))

    #Returns: (hsh,lbl,SparseVector(len(top_common_bigrams),Vector of index of bigrams+counts))
    test_hash_labels_common_bigrams = test_hash_labels_bigrams.map(lambda (hsh,bigrams):(hsh,extract_common_bigrams(bigrams,top_bigrams_broadcast.value)))
    #print hash_labels_bigrams.count()

    #4. Convert to dataframe
    testing_hash_labels_common_bigrams_df = test_hash_labels_common_bigrams.map(lambda (hsh,common_bigrams): (hsh,common_bigrams,float(1)))
    schema = StructType([StructField('hash',StringType(),True),StructField('features',VectorUDT(),True),StructField('label',DoubleType(),True)])   
    testingDF = testing_hash_labels_common_bigrams_df.toDF(schema)
    testingDF = testingDF.withColumn('features',testingDF.features.cast(VectorUDT()))
    print "testingDF created!!"

#-------------------------------------------------------------------------------
# Steps: RF
#-------------------------------------------------------------------------------

    rf = RandomForestClassifier(numTrees=200,impurity='gini',maxDepth=8, maxBins=32,seed=42)
    model = rf.fit(trainingDF)
    print "model fit!!"

    # Make predictions.
    result = model.transform(testingDF)
    print "model transformed!!"

    #save to csv
    result.select('hash','prediction').toPandas().to_csv('/home/ec2-user/predictionWithoutZipWithUnique2.csv',header=False,index=False)
    print "results written to csv"

    spark.stop()
    #get accuracy
    #predictionAndLabels = predictions.select("prediction", "label")
    #evaluator = MulticlassClassificationEvaluator(metricName="accuracy")
    #print "Accuracy: " + str(evaluator.evaluate(predictionAndLabels))
    


if __name__ == "__main__":
    main()    

