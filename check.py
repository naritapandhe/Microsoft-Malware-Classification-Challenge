from pyspark.sql import SparkSession
from pyspark.mllib.linalg import SparseVector, VectorUDT, Vectors
from pyspark.sql.types import *
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
import sys


def get_name(x):
    str1 = x.split(',')
    print str1
    return (str1[0],int(float(str1[1])))

def get_predicted_label(hsh,allDict):
    print hsh
    print "hsh here!!"
    if hsh in allDict:
        return allDict[hsh]

if __name__ == '__main__':

#-------------------------------------------------------------------------------
# Initialize Spark
#-------------------------------------------------------------------------------
    #initialize spark session
    spark = SparkSession\
            .builder\
            .appName("Test")\
            .getOrCreate()
    sc = spark.sparkContext

#-------------------------------------------------------------------------------
# Read the training data and build the model
#-------------------------------------------------------------------------------
    #load data
    xtest = sc.textFile('./data/X_test.txt').zipWithIndex().map(lambda x:(x[1],x[0].encode('utf-8')))
    mytest = sc.textFile('/Users/admin/Documents/pythonworkspace/data-science-practicum/project-2/eatingnails-project2/finalLargePredictions.csv').map(lambda x:get_name(x)).collect()
    mytestbc = sc.broadcast(dict(mytest))
    print mytestbc.value
    print "dictionary!!"


    result = xtest.map(lambda (indx,hsh):(hsh, get_predicted_label(hsh,mytestbc.value))).map(lambda (hsh,label):label).collect()
    orig_stdout = sys.stdout
    f = file('finalLargePredictions.txt', 'w')
    sys.stdout = f

    for i in result:
        print i

    sys.stdout = orig_stdout
    f.close() 
   

    