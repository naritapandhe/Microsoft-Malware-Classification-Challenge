#imports
from pyspark.mllib.util import MLUtils
from pyspark.sql import SparkSession
import os
import shutil as sh
from collections import Counter
from pyspark.rdd import RDD
import numpy as np
from itertools import chain
from pyspark.sql import functions as sql_functions
import sys
from pyspark.sql.types import *
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.linalg import SparseVector,Vectors, VectorUDT
from pyspark.ml.evaluation import MulticlassClassificationEvaluator



class preprocessor_bytes(object):
     
    def __init__(self):
        self._get_most_common_bigrams()
        
    def get_binary_names(self,hsh):
        return 's3n://eds-uga-csci8360/data/project2/binaries/'+hsh+'.bytes'

    #Reads the most_common_bigrams
    def _get_most_common_bigrams(self):
        bigrams1 = list(np.load('./data/most_common_bigrams.npy'))
        bigrams2 = dict(enumerate(bigrams1))
        self.top_bigrams = dict((y,x) for x,y in bigrams2.iteritems())
        

    def _tokenize(self,docLines):
        docLines = docLines.replace("\r\n"," ")
        tokens = [word for word in docLines.split() if word!='??' and len(word)==2]
        return tokens


    def _extract_common_bigrams(self,allWords):
        common_bigrams = []
        for i in range(0,len(allWords)-2):
            bigram = int(allWords[i]+allWords[i+1],16)
            if bigram in self.top_bigrams:
                common_bigrams.append(self.top_bigrams[bigram])
    
        sparse_dic = dict(Counter(common_bigrams))
        common_bigrams_frequency = SparseVector(len(self.top_bigrams),sparse_dic)
        return common_bigrams_frequency  

    def hashes_and_labels(self,x_rdd,y_rdd=None,both=True) :
        x_rdd2 = x_rdd.zipWithIndex().map(lambda (hsh,indx):(indx,hsh)).cache()
        if both and y_rdd:
            y_rdd2 = y_rdd.zipWithIndex().map(lambda (lbl,indx):(indx,lbl))
            hash_and_labels = x_rdd2.join(y_rdd2)
        
        else:
            hash_and_labels = x_rdd2

        return x_rdd2, hash_and_labels


    def transform(self, hash_labels_words):
        hash_labels_tokenized = hash_labels_words.map(lambda (hsh,lbl,docLines):(hsh,lbl,self._tokenize(docLines)))
    
        #Returns: (hsh,lbl,SparseVector(len(top_common_bigrams),Vector of index of bigrams+counts))
        hash_labels_common_bigrams = hash_labels_tokenized.map(lambda (hsh,lbl,tokens):(hsh,lbl,self._extract_common_bigrams(tokens)))
        return hash_labels_common_bigrams

    def toDataFrame(self,hash_labels_common_bigrams):
        hash_labels_common_bigrams_df = hash_labels_common_bigrams.map(lambda (hsh,lbl,common_bigrams): (hsh,common_bigrams,float(lbl)))
        schema = StructType([StructField('hash',StringType(),True),StructField('features',VectorUDT(),True),StructField('label',DoubleType(),True)])   
        df = hash_labels_common_bigrams_df.toDF(schema)
        df = df.withColumn('features',df.features.cast(VectorUDT()))
        return df
    


def main():
        
#-------------------------------------------------------------------------------
# Initialize spark session
#-------------------------------------------------------------------------------
    spark = SparkSession\
            .builder\
            .appName("Test")\
            .getOrCreate()
    
    sc = spark.sparkContext

    sc._jsc.hadoopConfiguration().set("fs.s3n.awsAccessKeyId", "***REMOVED***")
    sc._jsc.hadoopConfiguration().set("fs.s3n.awsSecretAccessKey", "***REMOVED***")

    xTestFlag = False
    yTestFlag = False
#-------------------------------------------------------------------------------
# Read all the file names
#-------------------------------------------------------------------------------
    fileNames = (sys.argv)
    X_train_file = fileNames[1]
    y_train_file = fileNames[2]

    if(len(fileNames)>=4):
        X_test_file = fileNames[3]
        xTestFlag = True

    if(len(fileNames)==5):
        y_test_file = fileNames[4]
        yTestFlag = True

    
#-------------------------------------------------------------------------------
# Training Data
#-------------------------------------------------------------------------------
    #Get the most common bigrams
    preprocessor1 = preprocessor_bytes()
    
    x_train = sc.textFile(X_train_file,36)
    y_train = sc.textFile(y_train_file,36)
   
    #Put hash, labels together. Returns: (indx,hsh) (indx,lbl) => (indx,(hsh,lbl))
    x_train, hash_and_labels = preprocessor1.hashes_and_labels(x_train,y_train,True)
   
    #returns an array of all the training file names
    binary_training_fileNames = x_train.map(lambda (indx,hsh):preprocessor1.get_binary_names(hsh)).collect()
    binary_training_fileNames_broadCast = sc.broadcast(list(binary_training_fileNames))
    training_binaries = sc.wholeTextFiles(','.join(binary_training_fileNames_broadCast.value),36).zipWithIndex().map(lambda (words,indx):(indx,words))
   
    #returns: (index,(hash,label),(file_url,words)) => (hash,label,words)
    hash_labels_words = hash_and_labels.join(training_binaries).map(lambda (indx,((hsh,label),(furl,words))):(hsh,label,words))
    
    #tokenizes and extracts common bigrams
    hash_labels_common_bigrams = preprocessor1.transform(hash_labels_words)
    
    #4. Convert to dataframe
    trainingDF = preprocessor1.toDataFrame(hash_labels_common_bigrams)
    #trainingDF.write.save("./data1/train_small_bytes.parquet")
    print "trainingDF created!!"
    print trainingDF.show()
 
#-------------------------------------------------------------------------------
# Testing data
#-------------------------------------------------------------------------------
    if xTestFlag:
        #Get the most common bigrams
        print "in xTestFlag check1!!"
        preprocessor2 = preprocessor_bytes()

        x_test = sc.textFile(X_test_file,36)

        if yTestFlag:
            y_test = sc.textFile(y_test_file,36)
            x_test, test_hash_and_labels = preprocessor2.hashes_and_labels(x_test,y_test,True)

            #returns an array of all the testing file names
            binary_testing_fileNames = x_test.map(lambda (indx,hsh):preprocessor2.get_binary_names(hsh)).collect()
            binary_testing_fileNames_broadCast = sc.broadcast(list(binary_testing_fileNames))
            testing_binaries = sc.wholeTextFiles(','.join(binary_testing_fileNames_broadCast.value),36).zipWithIndex().map(lambda (words,indx):(indx,words))
            test_hash_labels_words = test_hash_and_labels.join(testing_binaries).map(lambda (indx,((hsh,label),(furl,words))):(hsh,label,words))

            
        else:
            y_test=None
            x_test, test_hash_and_labels = preprocessor2.hashes_and_labels(x_test,y_test,False)

            #returns an array of all the testing file names
            binary_testing_fileNames = x_test.map(lambda (indx,hsh):preprocessor2.get_binary_names(hsh)).collect()
            binary_testing_fileNames_broadCast = sc.broadcast(list(binary_testing_fileNames))
            testing_binaries = sc.wholeTextFiles(','.join(binary_testing_fileNames_broadCast.value),36).zipWithIndex().map(lambda (words,indx):(indx,words))
            test_hash_labels_words = test_hash_and_labels.join(testing_binaries).map(lambda (indx,((hsh),(furl,words))):(hsh,1,words))

        
        #tokenizes and extracts common bigrams
        test_hash_labels_common_bigrams = preprocessor2.transform(test_hash_labels_words)

        #4. Convert to dataframe
        testingDF = preprocessor2.toDataFrame(test_hash_labels_common_bigrams)
        #testingDF.write.save("./data1/test_small_bytes.parquet")
        print "testingDF created!!"
        print testingDF.show()

#-------------------------------------------------------------------------------
# Steps: RF
#-------------------------------------------------------------------------------
    rf = RandomForestClassifier(numTrees=200,impurity='gini',maxDepth=8, maxBins=32,seed=42)
    model = rf.fit(trainingDF)
    print "model fit!!"

    # Make predictions.
    result = model.transform(testingDF)
    print "model transformed!!"

    #save to csv
    result.select('hash','prediction').toPandas().to_csv('/home/ec2-user/finalLargePredictions.csv',header=False,index=False)
    print "results written to csv"

    #get accuracy
    predictionAndLabels = predictions.select("prediction", "label")
    evaluator = MulticlassClassificationEvaluator(metricName="accuracy")
    print "Accuracy: " + str(evaluator.evaluate(predictionAndLabels))
    spark.stop()
    
    

if __name__ == "__main__":
    main()    

