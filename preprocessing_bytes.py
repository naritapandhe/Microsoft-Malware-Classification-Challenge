#imports
from collections import Counter
import sys
from pyspark.sql import SparkSession
from pyspark.ml.linalg import SparseVector, VectorUDT, Vectors
from pyspark.sql.types import *
from pyspark.rdd import RDD, PipelinedRDD
import numpy as np



class preprocessor_bytes(object):
    '''
    preprocessor for .bytes files in Microsoft Malware Classification Challenge
    generates the following features:
      - counts of hexademical bigrams (e.g., '53 8F')

    parameters:
      - most_common_bigrams: boolean
        whether or not to filter out everything but the top 1000 bigrams from each class (default True)
        
    methods:
      - hashes_and_labels(X_rdd,y_rdd=None)
        conditionally combines rdd of hashes and rdd of lables into a single rdd
        parameters:
          - X_rdd: pyspark rdd
            rdd with hashes as rows
          - y_rdd: pyspark rdd
            rdd with labels as rows
        outputs:
          - rdd with (hash,label) as rows, if both hashes,labels are present
          - else, outputs only hashes, when labels are absent
          
      - get_s3_path(X_rdd)
        convert rdd of hashes to their associated S3 paths for .asm files
        parameters:
          - X_rdd: pyspark rdd
            rdd with hashes as rows
        outputs:
          - list of strings containing S3 paths to .asm files
        
      - transform(bytes, hashes_and_labels=None)
        extract features from .bytes files
        parameters:
          - bytes: pyspark rdd
            rdd with .bytes files as rows (generated using sc.WholeTextFiles)
          - hashes_and_labels: pyspark rdd (optional)
            rdd with (hash,label) as rows
        outputs:
          - spark dataframe with hash, features, and label for each row
    
    requires following non-standard python packages
      - numpy
    '''
     
    def __init__(self,most_common_bigrams=True):
        self._get_most_common_bigrams(most_common_bigrams)

    #Reads the most_common_bigrams
    def _get_most_common_bigrams(self,most_common_bigrams):
        self.most_common_bigrams = most_common_bigrams
        self.num_features = 65536
        if self.most_common_bigrams:
            self.bigrams = list(np.load('./data/most_common_bigrams.npy'))
            self.num_features = len(self.bigrams)

    #read the file URLs         
    def get_s3_path(self,X_rdd):
        '''
        Convert rdd of hashes to their associated S3 paths for .bytes files
        '''
        #Check input type
        if type(X_rdd) != RDD:
            raise TypeError("Arguments must be pySpark RDDs")
            
        s3_path = X_rdd.map(lambda row: 's3n://eds-uga-csci8360/data/project2/binaries/'+row+'.bytes').collect()
        #s3_path = X_rdd.map(lambda row: './subsetData/'+row+'.bytes').collect()
        return list(s3_path)

    def _term_frequency(self,bytes):
        '''
        get term frequency of 4-hexadecimal-character words
        '''
        #get hash
        docHash = bytes[0].encode('utf-8').rsplit('/',1)[1].split('.bytes')[0]

        docLines = bytes[1].replace("\r\n"," ")
        docTokens = [word for word in docLines.split() if word!='??' and len(word)==2]
        return (docHash,docTokens)

    def _extract_common_bigrams(self,tokens):
        #get hex bigrams and convert hex to int
        tf = np.zeros(65536)
        for idx in xrange(len(tokens)-1):
            word = tokens[idx]+tokens[idx+1]
            converted = int(word,16)
            tf[converted] += 1
        if self.most_common_bigrams:
            tf = tf[self.bigrams]
        return tf

    def hashes_and_labels(self,X_rdd,y_rdd=None):
        '''
        Combine rdd of hashes and rdd of lables into a single rdd
        '''
        #Check input type
        if type(X_rdd) != RDD:
            raise TypeError("Arguments must be pySpark RDDs")
        hashes = X_rdd.map(lambda docHash:docHash.encode("utf-8")).zipWithIndex().map(lambda x:(x[1],x[0])) 
       
        if y_rdd is not None:
            if type(y_rdd) != RDD:
                raise TypeError("Arguments must be pySpark RDDs")

            labels = y_rdd.map(lambda docHash:docHash.encode("utf-8")).zipWithIndex().map(lambda x:(x[1],x[0]))   
       
            #Join X_rdd(hashes) and y_rdd(labels)
            hash_and_labels = hashes.join(labels).map(lambda (idx,(docHash,label)):(docHash,label))

        else:
             hash_and_labels = hashes   
        return hash_and_labels     

    def transform(self,bytes,hashes_and_labels=None):
        '''
        extract features from .bytes files
        hashes_and_labels =  None, implies testing data is being used, which does not have any labels

        '''
        #Check input type
        if type(bytes) != RDD:
            raise TypeError("Arguments must be pySpark RDDs")
        if hashes_and_labels and type(hashes_and_labels) != RDD and type(hashes_and_labels) != PipelinedRDD:
            raise TypeError("Arguments must be pySpark RDDs")
        
        
        #get term frequencies
        bytes = bytes.map(lambda doc:self._term_frequency(doc)).map(lambda doc:(doc[0],self._extract_common_bigrams(doc[1]))).cache()

        #convert to sparse
        bytes = bytes.map(lambda (docHash,features): (docHash, SparseVector(self.num_features,np.nonzero(features)[0],features[features>0])))

        #check if labels exist
        if hashes_and_labels is not None:
            #combine 
            data = hashes_and_labels.join(bytes).map(lambda (docHash,(label,features)):(docHash,features,label))
            schema = StructType([StructField('hash',StringType(),True),StructField('features',VectorUDT(),True),StructField('label',StringType(),True)])
            data = data.toDF(schema)
            data = data.withColumn('label',data.label.cast(DoubleType()))
        
        else:
            #if no labels, just use X
            schema = StructType([StructField('hash',StringType(),True),StructField("features", VectorUDT(), True)])
            data = bytes.toDF(schema)

        return data   
 


'''
def main():
        
#-------------------------------------------------------------------------------
# Initialize spark session
#-------------------------------------------------------------------------------
    spark = SparkSession\
            .builder\
            .appName("Test")\
            .getOrCreate()
    
    sc = spark.sparkContext

    sc._jsc.hadoopConfiguration().set("fs.s3n.awsAccessKeyId", "AKIAI5AGW2NXUVOJ7L2A")
    sc._jsc.hadoopConfiguration().set("fs.s3n.awsSecretAccessKey", "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")

    xTestFlag = False
    yTestFlag = False
#-------------------------------------------------------------------------------
# Read all the file names
#-------------------------------------------------------------------------------
    fileNames = (sys.argv)
    X_train_file = fileNames[1]
    y_train_file = fileNames[2]

    if(len(fileNames)>=4):
        X_test_file = fileNames[3]
        xTestFlag = True

    if(len(fileNames)==5):
        y_test_file = fileNames[4]
        yTestFlag = True

    
#-------------------------------------------------------------------------------
# Training Data
#-------------------------------------------------------------------------------
    #Get the most common bigrams
    preprocessor1 = preprocessor_bytes()
    
    x_train = sc.textFile(X_train_file)
    y_train = sc.textFile(y_train_file)
   
    #Put hash, labels together. Returns: (indx,hsh) (indx,lbl) => (indx,(hsh,lbl))
    x_train, hash_and_labels = preprocessor1.hashes_and_labels(x_train,y_train,True)
   
    #returns an array of all the training file names
    binary_training_fileNames = x_train.map(lambda (indx,hsh):preprocessor1.get_binary_names(hsh)).collect()
    binary_training_fileNames_broadCast = sc.broadcast(list(binary_training_fileNames))
    training_binaries = sc.wholeTextFiles(','.join(binary_training_fileNames_broadCast.value)).zipWithIndex().map(lambda (words,indx):(indx,words))
    
    #returns: (index,(hash,label),(file_url,words)) => (hash,label,words)
    hash_labels_words = hash_and_labels.join(training_binaries).map(lambda (indx,((hsh,label),(furl,words))):(hsh,label,words))
    
    #tokenizes and extracts common bigrams
    hash_labels_common_bigrams = preprocessor1.transform(hash_labels_words,True)
    #print hash_labels_common_bigrams.take(1)
    
    #4. Convert to dataframe
    trainingDF = preprocessor1.toDataFrame(hash_labels_common_bigrams)
    trainingDF.write.save("./data1/train_small_tfidf_bytes.parquet")
    print "trainingDF created!!"
    print trainingDF.show()
   
#-------------------------------------------------------------------------------
# Testing data
#-------------------------------------------------------------------------------
    if xTestFlag:
        #Get the most common bigrams
        #preprocessor1 = preprocessor_bytes()

        x_test = sc.textFile(X_test_file,36)

        if yTestFlag:
            y_test = sc.textFile(y_test_file,36)
            x_test, test_hash_and_labels = preprocessor1.hashes_and_labels(x_test,y_test,True)

            #returns an array of all the testing file names
            binary_testing_fileNames = x_test.map(lambda (indx,hsh):preprocessor1.get_binary_names(hsh)).collect()
            binary_testing_fileNames_broadCast = sc.broadcast(list(binary_testing_fileNames))
            testing_binaries = sc.wholeTextFiles(','.join(binary_testing_fileNames_broadCast.value),36).zipWithIndex().map(lambda (words,indx):(indx,words))
            test_hash_labels_words = test_hash_and_labels.join(testing_binaries).map(lambda (indx,((hsh,label),(furl,words))):(hsh,label,words))

            
        else:
            y_test=None
            x_test, test_hash_and_labels = preprocessor1.hashes_and_labels(x_test,y_test,False)

            #returns an array of all the testing file names
            binary_testing_fileNames = x_test.map(lambda (indx,hsh):preprocessor1.get_binary_names(hsh)).collect()
            binary_testing_fileNames_broadCast = sc.broadcast(list(binary_testing_fileNames))
            testing_binaries = sc.wholeTextFiles(','.join(binary_testing_fileNames_broadCast.value),36).zipWithIndex().map(lambda (words,indx):(indx,words))
            test_hash_labels_words = test_hash_and_labels.join(testing_binaries).map(lambda (indx,((hsh),(furl,words))):(hsh,1,words))

        
        #tokenizes and extracts common bigrams
        test_hash_labels_common_bigrams = preprocessor1.transform(test_hash_labels_words,False)

        #4. Convert to dataframe
        testingDF = preprocessor1.toDataFrame(test_hash_labels_common_bigrams)
        testingDF.write.save("./data1/test_small_tfidf_bytes.parquet")
        print "testingDF created!!"
        print testingDF.show()

#--------------------------------------------------------------------------------------------
# Random Forests: 
#    - If only X_test is given, then the below code just builds a RF model
#    - If both X_test, y_test are given, only then accuracies of predictions are calculated
#--------------------------------------------------------------------------------------------
    
    rf = RandomForestClassifier(numTrees=200,impurity='gini',maxDepth=8, maxBins=32,seed=42)
    model = rf.fit(trainingDF)
    print "model fit!!"

    if yTestFlag:
 
        # Make predictions.
        result = model.transform(testingDF)
        print "model transformed!!"

        #save to csv
        result.select('hash','prediction').toPandas().to_csv('/home/ec2-user/finalLargePredictions.csv',header=False,index=False)
        print "results written to csv"

        #get accuracy
        predictionAndLabels = result.select("prediction", "label")
        evaluator = MulticlassClassificationEvaluator(metricName="accuracy")
        print "Accuracy: " + str(evaluator.evaluate(predictionAndLabels))
        spark.stop()
   
    

if __name__ == "__main__":
    main()    

'''