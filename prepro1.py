from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import *
from pyspark.rdd import RDD
from pyspark.ml.linalg import SparseVector, VectorUDT
import numpy as np
import urllib
from itertools import chain
from pyspark.sql import Row
from pyspark.ml.feature import NGram
import re
from pyspark.sql import functions as sql_functions

def get_binary_file(hash,label):
    fileURL = 'https://s3.amazonaws.com/eds-uga-csci8360/data/project2/binaries/I1kgetcxWaBhLoMJ8sZp.bytes' % hash
    return (hash,fileURL,label)

def read_data_from_url(url):
    return urllib.urlopen(url)

def read_document_data(url,label):
    #read the entire file
    documentText = read_data_from_url(url)

    #create an array of documents
    data = []
    for line in documentText:
        data.append([word.encode('utf-8') for word in line.strip().split(' ') if len(word)==2 and word!="??"])

    z = list(chain.from_iterable(data))
    return (z,label)


"""
class preprocessor(object):
    def __init__(self):
        #load list of most common bigrams from training set
        bigrams = list(np.load('./data/most_common_bigrams.npy'))
        bigrams = dict(enumerate(bigrams))
        self.top_bigrams = {y:x for x,y in bigrams.iteritems()}
    
    def _term_frequency(self,path):
        '''
        get term frequency of 4-hexadecimal-character words
        '''
        #get byte file from hash and tokenize
                
        #add bigrams to sparse dic
        sparse_dic = {}
        for idx in xrange(len(tokens)-1):
            word = tokens[idx]+tokens[idx+1]
            converted = int(word,16)
            if converted in self.top_bigrams:
                key = self.top_bigrams[converted]
                if key in sparse_dic:
                    sparse_dic[key] += 1.
                else:
                    sparse_dic[key] = 1.

        tf = SparseVector(len(self.top_bigrams),sparse_dic)
        return tf

    def transform(self,X_rdd,y_rdd=None):
        '''
        given X RDD (and optionally y RDD), output dataframe with term frequency feature vector and labels
        '''    
        #check input type
        if type(X_rdd) != RDD:
            raise TypeError("Arguments must be pySpark RDDs")
        if y_rdd and type(y_rdd) != RDD:
            raise TypeError("Arguments must be pySpark RDDs")
        
        #convert X to URL paths
        X = X_rdd.map(lambda x: 's3n://eds-uga-csci8360/data/project2/' + x.replace('\n','') + '.bytes')
        X = X.map(self._term_frequency)
        
        #check if labels exist
        if y_rdd:
            #combine X and y into single dataframe
            X = X.zipWithIndex().map(lambda r: (r[1],r[0]))
            y = y_rdd.zipWithIndex().map(lambda r: (r[1],r[0]))
            data = X.join(y).map(lambda r: r[1])
            schema = StructType([StructField('features',VectorUDT(),True),StructField('label',StringType(),True)])
            data = data.toDF(schema)
            data = data.withColumn('label',data.label.cast(DoubleType()))
        
        else:
            X = X.map(lambda row: [row])
            schema = StructType([StructField("features", VectorUDT(), True)])
            data = X.toDF(schema)
            
        return data
"""
if __name__ == '__main__':

    #initialize spark session
    spark = SparkSession\
            .builder\
            .appName("Test")\
            .getOrCreate()
    sc = spark.sparkContext
    
    #paths to training data
    X_file = "./data/X_train_small.txt"
    y_file = "./data/y_train_small.txt"
    
    
    #read all the hashes
    hashValues = sc.textFile(X_file,50).map(lambda hash:hash.encode("utf-8")).zipWithIndex().map(lambda x:(x[1],x[0]))
    
    #read all the labels
    labelValues = sc.textFile(y_file,50).map(lambda hash:hash.encode("utf-8")).zipWithIndex().map(lambda x:(x[1],x[0]))
    #labelValuesBroadCast = sc.broadcast(labelValues.collectAsMap())

    #join labels and hashes
    hashAndLabel = hashValues.join(labelValues).map(lambda (x,(y1,y2)):(y1,y2)) 
    
    #Put eveyrhting together: hash,binaryUrl,label
    hashBinaryAndLabel = hashAndLabel.map(lambda (hash,label): get_binary_file(hash,label))

    #Extract: binaryUrl,label
    binaryUrlAndLabel = hashBinaryAndLabel.map(lambda (x,y,z):(y,z))
    
    #Download the binaries for now:
    binaryDataAndLabel = binaryUrlAndLabel.map(lambda (x,y):read_document_data(x,y))
    
    #create a dataframe
    data = binaryDataAndLabel.map(lambda p: Row(words=p[0], label=p[1].encode('utf-8')))

    #Infer the schema, and register the DataFrame as a table.
    schemaDataFrame = spark.createDataFrame(data)
    schemaDataFrame.createOrReplaceTempView("data")

    ngram = NGram(inputCol="words", outputCol="ngrams")
    ngramDataFrame1 = ngram.transform(schemaDataFrame)
    ngramDataFrame2 = ngramDataFrame1.select("label", "ngrams")
    


    """        
    binaries = ['s3n://eds-uga-csci8360/data/project2/binaries/%s.bytes' % x.rstrip() for x in binaries]
    rdds = []

    for i in range(len(binaries)):
        sparse_dic = sc.textFile(binaries[i])\
            .map(lambda line: [str(word) for word in line.split(' ') if len(word)==2 and word!="??"])\
            .map(lambda line: zip(line,line[1:]))\
            .map(lambda bigrams: [int(pair[0]+pair[1],16) for pair in bigrams if int(pair[0]+pair[1],16) in top_bigrams])\
            .flatMap(lambda key: key)\
            .map(lambda key: (top_bigrams[key],1))\
            .reduceByKey(lambda acc, w: acc + w).collectAsMap()
        tf = SparseVector(len(top_bigrams),sparse_dic)
        rdds.append(sc.parallelize([i,tf]))
    
    print rdds
    """

    '''    
    for i in range(len(binaries)):
        sparse_dic = sc.textFile(binaries[i])\
            .map(lambda line: [str(word) for word in line.split(' ') if len(word)==2 and word!="??"])\
            .map(lambda line: zip(line,line[1:]))\
            .map(lambda bigrams: [int(pair[0]+pair[1],16) for pair in bigrams if int(pair[0]+pair[1],16) in top_bigrams])\
            .flatMap(lambda key: key)\
            .map(lambda key: (top_bigrams[key],1))\
            .reduceByKey(lambda acc, w: acc + w).collectAsMap()
        tf = SparseVector(len(top_bigrams),sparse_dic)
        rdds.append(sc.parallelize([i,tf]))
    
    X = sc.union(rdds)
    y = y_file.zipWithIndex().map(lambda r: (r[1],r[0]))
    data = X.join(y).map(lambda r: r[1])
    schema = StructType([StructField('features',VectorUDT(),True),StructField('label',StringType(),True)])
    data = data.toDF(schema)
    data = data.withColumn('label',data.label.cast(DoubleType()))
    
    print data.show()
    
    try:
        data.write.save("./data/train_small.parquet")
    except:
        pass
    
    #preprocess data
    preprocessor1 = preprocessor()
    data = preprocessor1.transform(X_file,y_file)
    
    print data.show()
    
    #save to parquet
    try:
        data.write.save("./data/train_small.parquet")
    except:
        pass
        
    #paths to test data
    X_file = "./data/X_test_small.txt"
    y_file = "./data/y_test_small.txt"
    X_file = sc.textFile(X_file)
    y_file = sc.textFile(y_file)
    
    #preprocess data
    preprocessor2 = preprocessor()
    data = preprocessor2.transform(X_file,y_file)
    
    print data.show()
    
    #save to parquet
    try:
        data.write.save("./data/test_small.parquet")
    except:
        pass
    '''