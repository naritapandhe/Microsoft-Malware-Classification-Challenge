from pyspark.sql import SparkSession
from pyspark.sql.types import StringType, StructType, StructField, FloatType
import urllib2, requests
import StringIO, zipfile
import sys
from time import sleep

class file_properties(object):
    
    '''raw_ratios = False
    compressed_ratios = False
    r2c_ratio = False'''

    def __init__(self):#, raw_ratios, compressed_ratios, r2c_ratio):
        '''self.raw_ratios = raw_ratios
        self.compressed_ratios = compressed_ratios
        self.r2c_ratio = r2c_ratio'''
        pass
        
    def _get_compressed_size(self, file, bytes_flag):

        file_ext = '.asm'
        if bytes_flag:
            file_ext = '.bytes'

        zipped_file = StringIO.StringIO()
        zipped_file_object = zipfile.ZipFile(zipped_file, 'w', zipfile.ZIP_DEFLATED)
                                
        # The file 'file' is witten to the archive zipped_file_here
        zipped_file_object.writestr('file' + file_ext, file.getvalue()) #try writestr with get value
        zipped_file_object.close()

        size = zipped_file_object.getinfo('file' + file_ext).compress_size
        
        return size

    def _get_size_ratios(self, data):

        hash_list = list(data)
        raw_sizes = [0.00, 0.00] # zeroth position to hold asm file size
        #compressed_sizes = [0.00, 1.00]
        bytes_flag = False

        return_this = []

        for hash_name in hash_list:

            print hash_name

            urls = ['https://s3.amazonaws.com/eds-uga-csci8360/data/project2/metadata/' + hash_name + '.asm', \
            'https://s3.amazonaws.com/eds-uga-csci8360/data/project2/binaries/' + hash_name + '.bytes']
                    
            for url in urls:

                response = None      
                request_object = requests.head(url)
                size = request_object.headers['Content-Length']
            
                '''else:
                        response = urllib2.urlopen(url)
                        info = str(response.info())
                        s = info.find('Content-Length:')
                        e = info.find('Server')
                        size = info[(s+15):e].lstrip().rstrip()'''
                
                if url.endswith('s'):
                    raw_sizes[1] = (float(size))
                    bytes_flag = True
                else:
                    raw_sizes[0] = (float(size))
            
                '''if self.compressed_ratios:

                    # -------------------------------------------------------------------- #
                    # Get the size of the compressed file. All processing is done in-memory
                    # without the need of disk IO, god bless StringIO
                    # -------------------------------------------------------------------- #
            
                    # This file holds the response data
                    in_memory_file = StringIO.StringIO(response.read())
                    in_memory_file.write(response.read())
            
                    size_compressed = self._get_compressed_size(in_memory_file, bytes_flag)
                    
                    if bytes_flag:
                        compressed_sizes[1] = (float(size_compressed))
                    else:
                        compressed_sizes[0] = (float(size_compressed))
            
                    # do not delete
                    in_memory_file.close()
                    response.close()'''
            
            # return list of ratios with first postition having ratio of raw file sizes and 
            # second with compressed file sizes

            ab = raw_sizes[0] / raw_sizes[1]
            #abc = compressed_sizes[0] / compressed_sizes[1]
            #if abc == 0.00:
            #    abc = 1.00
            
            #sleep to not inundate the server and avoid getting 500 - internal server error
            sleep(0.01)    
            
            print [hash_name, ab]#, abc, ab/abc]

            return_this.append((hash_name, ab))#, abc, ab/abc))
        
        return return_this

    def transform(self, x_rdd, y_rdd = None):
        
        x_processed_rdd = x_rdd.mapPartitions(self._get_size_ratios, preservesPartitioning = True)
        x_processed_rdd = x_processed_rdd.map(lambda r : [r[0], r[1]])#, r[2], r[3]])
        
        return x_processed_rdd

if __name__ == '__main__':

    x_train_file = 's3n://eds-uga-csci8360/data/project2/labels/X_train.txt'
    x_test_file = 's3n://eds-uga-csci8360/data/project2/labels/X_test.txt'

    spark = SparkSession\
            .builder\
            .appName("Test")\
            .getOrCreate()

    sc = spark.sparkContext
    sc.setLogLevel("OFF")

    ***REMOVED***
    ***REMOVED***

    x_train = sc.textFile(x_train_file, 40)
    x_test = sc.textFile(x_test_file, 40)

    rdds = [x_train, x_test]

    train_file_flag = True

    for rdd in rdds:
        properties = file_properties()
        '''raw_ratios = sys.argv[1], \
            compressed_ratios = sys.argv[2], \
            r2c_ratio = sys.argv[3])'''

        #debug
        print "transforming now"

        properties_rdd = properties.transform(rdd)
        
        schema = StructType([StructField('hash', StringType(), True), \
            StructField('ab_ratio', FloatType(), True)])#, \
        '''StructField('abc_ratio', FloatType(), True), \
            StructField('ab2abc_ratio', FloatType(), True)])'''

        
        properties_df = properties_rdd.toDF(schema)

        '''if not properties.raw_ratios:
            properties_df = properties_df.drop('ab_ratio')
        if not properties.compressed_ratios:
            properties_df = properties_df.drop('abc_ratio')
        if not properties.r2c_ratio:
            properties_df = properties_df.drop('ab2abc_ratio')'''

        # write to parquet
        try:
            if train_file_flag:
                properties_df.write.save('./data/train_properties_no_compression.parquet')
                train_file_flag = False
            else:
                properties_df.write.save('./data/test_properties_no_compression.parquet')
        except:
            print 'could not write parquet file'
            raise
    spark.stop()