from pyspark.sql import SparkSession
from pyspark.ml.linalg import SparseVector, VectorUDT, Vectors
from pyspark.sql.types import *
from pyspark.rdd import RDD
import numpy as np
import re

class preprocessor_asm(object):

    def __init__(self,min_df=30):
        self.min_df = min_df

    def _tokenize(self,row):
        '''
        convert raw text into tokens
        '''
        
        #get hash
        hash = row[0].encode('utf-8').rsplit('/',1)[1].split('.asm')[0]
        
        #filter out important words
        prefixes = ['HEADER:','.text:','.Pav:','.idata:','.data:','.bss:','.rdata:','.edata:','.rsrc:','.tls:','.reloc:']
        opcodes = ['jmp', 'mov', 'retf', 'push', 'pop', 'xor', 'retn', 'nop', 'sub', 'inc', 'dec', 'add', \
                   'imul', 'xchg', 'or', 'shr', 'cmp', 'call', 'shl', 'ror', 'rol', 'jnb']
        keywords = ['.dll','std::',':dword']
        keywords2 = ['FUNCTION','call']
        tokens = re.sub(r'\n|\r|\t',' ',row[1]).split()
        opcode_list = []
        filtered = []
        filtered.append('HEADER:') # add Header because first word of each file is skipped
        for i in xrange(1,len(tokens)-1): #skip first and last word so we can check previous and following word during iter
            if any(tokens[i]==opcode for opcode in opcodes):
                for opcode in opcodes:
                    if tokens[i]==opcode:
                        filtered.append(opcode)
                        opcode_list.append(opcode)
                        break
            if any(prefix in tokens[i] for prefix in prefixes):
                for prefix in prefixes:
                    if prefix in tokens[i]:
                        filtered.append(prefix)
                        break
            elif any(keyword in tokens[i] for keyword in keywords):
                filtered.append(tokens[i])
            elif any(tokens[i]==keyword for keyword in keywords2):
                bigram = tokens[i] + ' ' + tokens[i+1]
                filtered.append(bigram)
            elif tokens[i] == '__stdcall':
                bigram = tokens[i] + ' ' + tokens[i+1].partition("(")[0]
                filtered.append(bigram)
                filtered.append(tokens[i-1])
            elif tokens[i] == 'db' and tokens[i+1][0] == "'":
                bigram = tokens[i] + ' ' + tokens[i+1]
                filtered.append(bigram)

        #add opcode bigrams
        for i in range(len(opcode_list)-1):
            bigram = opcode_list[i] + ' ' + opcode_list[i+1]
            filtered.append(bigram)

        return (hash,filtered)
    
    def _tf_idf(self,row):
        '''
        convert row of word token counts into sparse vector of tfidf frequencies
        '''
        sparse_dic = {}
        df_dic = {}
        for word in row[1]:
            if word in self.dictionary:
                if self.dictionary[word] in sparse_dic:
                    sparse_dic[self.dictionary[word]] += 1.
                else:
                    sparse_dic[self.dictionary[word]] = 1.
                if word in self.doc_freq:
                    df_dic[self.dictionary[word]] = self.doc_freq[word]
                else:
                    df_dic[self.dictionary[word]] = 1
        for key in sparse_dic:
            sparse_dic[key] = (1+np.log(sparse_dic[key]))*(np.log10(float(self.doc_count)/df_dic[key]))                    
        tfidf = SparseVector(len(self.dictionary),sparse_dic)
        return (row[0],tfidf)
    
    def hashes_and_labels(self,X_rdd,y_rdd):

        #Check input type
        if type(X_rdd) != RDD:
            raise TypeError("Arguments must be pySpark RDDs")
        if type(y_rdd) != RDD:
            raise TypeError("Arguments must be pySpark RDDs")

        hashes = X_rdd.map(lambda hash:hash.encode("utf-8")).zipWithIndex().map(lambda x:(x[1],x[0])) 
        labels = y_rdd.map(lambda hash:hash.encode("utf-8")).zipWithIndex().map(lambda x:(x[1],x[0]))   
       
        #Join X_rdd(hashes) and y_rdd(labels)
        hash_and_labels = hashes.join(labels)
        return hash_and_labels
        
    def get_s3_path(self,X_rdd) :
        """
        Given X RDD, which has the hashes, return a list of names of the binaries
        """
        s3_path = X_rdd.map(lambda row: 's3n://eds-uga-csci8360/data/project2/metadata/'+row+'.asm').collect()
        return list(s3_path)
        
    def transform(self,metadata,hashes_and_labels=None,train=True):
    
        #word tokenization
        X = metadata.map(self._tokenize).cache()
        
        #create dictionary of words
        if train:
            self.dictionary = X.map(lambda row: row[1]).flatMap(lambda word: word).map(lambda word: (word,1)).reduceByKey(lambda acc, w: acc + w).filter(lambda x: x[1]>=self.min_df).collectAsMap()
            self.dictionary = dict(zip(self.dictionary,xrange(len(self.dictionary))))
            
            #populate word count dictionary
            self.doc_freq = X.map(lambda row: set(row[1])).flatMap(lambda word: word).map(lambda word: (word,1)).reduceByKey(lambda acc, w: acc + w).filter(lambda x: x[1]>=2).collectAsMap()
            self.doc_count = X.count()

        #create word vectors
        X = X.map(self._tf_idf)
        
        #check if labels exist
        if hashes_and_labels:
            #combine X and y into single dataframe
            X = X.zipWithIndex().map(lambda r: (r[1],r[0]))
            data = hashes_and_labels.join(X).map(lambda (index,((hash,label),(hash2,features))):(hash,features,label))
            schema = StructType([StructField('hash',StringType(),True),StructField('features',VectorUDT(),True),StructField('label',StringType(),True)])
            data = data.toDF(schema)
            data = data.withColumn('label',data.label.cast(DoubleType()))
        
        else:
            schema = StructType([StructField('hash',StringType(),True),StructField("features", VectorUDT(), True)])
            data = X.toDF(schema)
            
        return data
      
if __name__ == '__main__':        

    #initialize spark session
    spark = SparkSession\
            .builder\
            .appName("Test")\
            .config('spark.sql.warehouse.dir', 'file:///C:/')\
            .getOrCreate()
    sc = spark.sparkContext
    
    #paths to training data
    X_file = "./data/X_train_small.txt"
    y_file = "./data/y_train_small.txt"
    X_file = sc.textFile(X_file,20)
    y_file = sc.textFile(y_file,20)
    
    #preprocess data
    preprocessor1 = preprocessor_asm()
    hashes_and_labels = preprocessor1.hashes_and_labels(X_file,y_file)
    s3_path = preprocessor1.get_s3_path(X_file)
    metadata = sc.wholeTextFiles(','.join(s3_path),20)
    #metadata = sc.wholeTextFiles('./data/metadata/*',20)
    
    data = preprocessor1.transform(metadata,hashes_and_labels)
    
    print data.show()

    #save to parquet
    try:
        data.write.save("./data/train_small_asm.parquet")
    except:
        pass
        
    #paths to test data
    X_file = "./data/X_test_small.txt"
    y_file = "./data/y_test_small.txt"
    X_file = sc.textFile(X_file,20)
    y_file = sc.textFile(y_file,20)
    
    #preprocess data
    hashes_and_labels = preprocessor1.hashes_and_labels(X_file,y_file)
    s3_path = preprocessor1.get_s3_path(X_file)
    metadata = sc.wholeTextFiles(','.join(s3_path),20)
    
    data = preprocessor1.transform(metadata,hashes_and_labels,train=False)
    
    print data.show()
    
    #save to parquet
    try:
        data.write.save("./data/test_small_asm.parquet")
    except:
        pass