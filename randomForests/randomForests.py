from pyspark.mllib.tree import RandomForest, RandomForestModel
from pyspark.mllib.util import MLUtils
from pyspark.mllib.linalg import Vectors
from pyspark.mllib.regression import LabeledPoint
from pyspark.sql import SparkSession
import os
import shutil as sh


if __name__ == '__main__':

#-------------------------------------------------------------------------------
# Initialize Spark
#-------------------------------------------------------------------------------
    #initialize spark session
    spark = SparkSession\
            .builder\
            .appName("Test")\
            .getOrCreate()
    sc = spark.sparkContext

#-------------------------------------------------------------------------------
# Read the training data and build the model
#-------------------------------------------------------------------------------

    #reading the train dataframes
    trainingDF = spark.read.load("../data/train_small.parquet")     

    #convert every row to LabeledPoint
    transformedTrainingRDD = (trainingDF.rdd
                             .map(lambda row: LabeledPoint(int(row.label)-1,row.features))
                             )
    #print transformedTrainingRDD.show()
    
    #Save the RDD in LibSVM format, as Naive Bayes reads in the same format
    MLUtils.saveAsLibSVMFile(transformedTrainingRDD,"trainingLibsvmfile")
    training = MLUtils.loadLibSVMFile(sc, "trainingLibsvmfile/*")
    print "trainingLibsvmfile created!!"

    
    # Train a RandomForest model.
    #  Empty categoricalFeaturesInfo indicates all features are continuous.
    #  Note: Use larger numTrees in practice.
    #  Setting featureSubsetStrategy="auto" lets the algorithm choose.
    model = RandomForest.trainClassifier(training, numClasses=10, categoricalFeaturesInfo={},
                                     numTrees=24, featureSubsetStrategy="auto",
                                     impurity='gini', maxDepth=4, maxBins=32)
    print "Model built!!"

#-------------------------------------------------------------------------------
# Read the testing data and predict
#-------------------------------------------------------------------------------
    #reading the test dataframes
    testingDF = spark.read.load("../data/test_small.parquet")     
    
    #convert every row to LabeledPoint
    transformedTestRDD = (testingDF.rdd
                             .map(lambda row: LabeledPoint(int(row.label)-1,row.features)))
    
    
    MLUtils.saveAsLibSVMFile(transformedTestRDD,"testingLibsvmfile")
    testingData = MLUtils.loadLibSVMFile(sc, "testingLibsvmfile/*")
    print "testingLibsvmfile created!!"

    # Evaluate model on test instances and compute test error
    print "Predicting...."
    predictions = model.predict(testingData.map(lambda x: x.features))
    labelsAndPredictions = testingData.map(lambda lp: lp.label).zip(predictions)
    testIncorrect = labelsAndPredictions.filter(lambda (v, p): v != p).count()
    testCorrect = labelsAndPredictions.filter(lambda (v, p): v == p).count()
    print('Total Count: '+str(testingData.count()))
    print('Correct Count: '+str(testCorrect))
    print('Incorrect Count: '+str(testIncorrect))
    print('Learned classification forest model:')
    print(model.toDebugString())
    