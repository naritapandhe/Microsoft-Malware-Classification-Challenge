from pyspark.sql import SparkSession
from pyspark.ml.linalg import SparseVector, VectorUDT, Vectors
from pyspark.sql.types import *
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.feature import ChiSqSelector
from pyspark.ml.classification import RandomForestClassifier
from preprocessing_bytes import preprocessor_bytes
from preprocessing_asm import preprocessor_asm

#initialize spark session
spark = SparkSession\
        .builder\
        .appName("Malware Random Forests")\
        .getOrCreate()
sc = spark.sparkContext

sc._jsc.hadoopConfiguration().set("fs.s3n.awsAccessKeyId", "***REMOVED***")
sc._jsc.hadoopConfiguration().set("fs.s3n.awsSecretAccessKey", "***REMOVED***")

#paths to training data
X_file = "./data/X_train.txt"
y_file = "./data/y_train.txt"
X_file = sc.textFile(X_file,40)
y_file = sc.textFile(y_file,40)

#get train bytes features
preprocessor_b = preprocessor_bytes(most_common_bigrams=True)
hashes_and_labels = preprocessor_b.hashes_and_labels(X_file,y_file)
s3_path = preprocessor_b.get_s3_path(X_file)
bytes = sc.wholeTextFiles(','.join(s3_path),40)
train_bytes = preprocessor_b.transform(bytes,hashes_and_labels)

#get asm features
preprocessor_a = preprocessor_asm()
s3_path = preprocessor_a.get_s3_path(X_file)
metadata = sc.wholeTextFiles(','.join(s3_path),40)
train_asm = preprocessor_a.transform(metadata,hashes_and_labels)

#paths to test data
X_file = "./data/X_test.txt"
X_file = sc.textFile(X_file,40)

#get test bytes data
hashes_and_labels = preprocessor_b.hashes_and_labels(X_file,y_file)
s3_path = preprocessor_b.get_s3_path(X_file)
bytes = sc.wholeTextFiles(','.join(s3_path),40)
test_bytes = preprocessor_b.transform(bytes)

#get test asm data
s3_path = preprocessor1.get_s3_path(X_file)
metadata = sc.wholeTextFiles(','.join(s3_path),40)
data = preprocessor1.transform(metadata,train=False)

#to rdd
train_bytes = train_bytes.rdd.map(lambda (hash,feats,label):(hash,(feats,label)))
test_bytes = test_bytes.rdd.map(lambda (hash,feats,label):(hash,(feats,label)))
train_asm = train_asm.rdd.map(lambda (hash,feats,label):(hash,(feats,label)))
test_asm = test_asm.rdd.map(lambda (hash,feats,label):(hash,(feats,label)))

#merge bytes and asm data
train = train_bytes.join(train_asm).map(lambda (hash,((bytes,label),(asm,label2))):(bytes,asm,label))
test = test_bytes.join(test_asm).map(lambda (hash,((bytes,label),(asm,label2))):(bytes,asm,label))
schema = StructType([StructField('bytes',VectorUDT(),True),StructField('asm',VectorUDT(),True),StructField('label',StringType(),True)])
train = train.toDF(schema)
train = train.withColumn('label',train.label.cast(DoubleType()))
test = test.toDF(schema)
test = test.withColumn('label',test.label.cast(DoubleType()))
            
#merge bytes and asm features
assembler = VectorAssembler(inputCols=["bytes", "asm"],outputCol="features")
train = assembler.transform(train)
test = assembler.transform(test)

#rf classifier
rf = RandomForestClassifier(numTrees=200,maxDepth=8,maxBins=48,maxMemoryInMB=512,seed=1)
model = rf.fit(train)
result = model.transform(test)

#save to csv
result.select("prediction").toPandas().astype(int).to_csv('prediction.csv',header=False,index=False)

spark.stop()
