from pyspark.sql import SparkSession
from pyspark.ml.linalg import SparseVector, VectorUDT, Vectors
from pyspark.sql.types import *
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.feature import ChiSqSelector
from pyspark.ml.classification import RandomForestClassifier
from preprocessing_bytes_narita import preprocessor_bytes
from preprocessing_asm import preprocessor_asm
import sys

def get_predicted_label(docHash,allDict):
    if docHash in allDict:
        return allDict[docHash]

def save_predicted_labels(predictions,fileName):
    orig_stdout = sys.stdout
    f = file(fileName, 'w')
    sys.stdout = f
    for i in predictions:
        print i

    sys.stdout = orig_stdout
    f.close() 


def main():

    #initialize spark session
    spark = SparkSession\
            .builder\
            .appName("Malware Random Forests")\
            .getOrCreate()
    sc = spark.sparkContext

    sc._jsc.hadoopConfiguration().set("fs.s3n.awsAccessKeyId", "***REMOVED***")
    sc._jsc.hadoopConfiguration().set("fs.s3n.awsSecretAccessKey", "***REMOVED***")

    #read file names
    fileNames = (sys.argv)
    X_train = sc.textFile(fileNames[1],40)
    y_train = sc.textFile(fileNames[2],40)

    #paths to test data
    X_test = sc.textFile(fileNames[3],40) if len(fileNames)>=4 else None
    y_test = sc.textFile(fileNames[4],40) if len(fileNames)==5 else None


    #get train bytes features
    preprocessor_b = preprocessor_bytes(most_common_bigrams=True)
    hashes_and_labels = preprocessor_b.hashes_and_labels(X_train,y_train)
    s3_path = preprocessor_b.get_s3_path(X_train)
    bytes = sc.wholeTextFiles(','.join(s3_path),40)
    train_bytes = preprocessor_b.transform(bytes,hashes_and_labels)
    print "train_bytes done!!!!!!!"
    
    #get asm features
    preprocessor_a = preprocessor_asm()
    s3_path = preprocessor_a.get_s3_path(X_train)
    metadata = sc.wholeTextFiles(','.join(s3_path),40)
    train_asm = preprocessor_a.transform(metadata,hashes_and_labels,train=True)
    print "train_asm done!!!!!!!"
    
    #get test bytes data
    hashes_and_labels = preprocessor_b.hashes_and_labels(X_test,y_test)
    s3_path = preprocessor_b.get_s3_path(X_test)
    bytes = sc.wholeTextFiles(','.join(s3_path),40)
    test_bytes = preprocessor_b.transform(bytes,hashes_and_labels) if y_test is not None else preprocessor_b.transform(bytes)
    print "test_bytes done!!!!!!!"
    
    #get test asm data
    s3_path = preprocessor_a.get_s3_path(X_test)
    metadata = sc.wholeTextFiles(','.join(s3_path),40)
    test_asm = preprocessor_a.transform(metadata,hashes_and_labels,train=False)
    print "test_asm done!!!!!!!"
    
    #chisqr feature selector
    selector1 = ChiSqSelector(numTopFeatures=150, outputCol="selectedFeatures")
    selectormodel1 = selector1.fit(train_bytes)
    train_bytes = selectormodel1.transform(train_bytes)
    test_bytes = selectormodel1.transform(test_bytes)
    print "ChiSqSelector bytes done!!!!!!!"
   
    selector2 = ChiSqSelector(numTopFeatures=150, outputCol="selectedFeatures")
    selectormodel2 = selector2.fit(train_asm)
    train_asm = selectormodel2.transform(train_asm)
    test_asm = selectormodel2.transform(test_asm)
    print "ChiSqSelector asm done!!!!!!!"
    
    #to rdd
    train_bytes = train_bytes.select('hash','selectedFeatures','label').rdd.map(lambda (hash,feats,label):(hash,(feats,label)))
    train_asm = train_asm.select('hash','selectedFeatures','label').rdd.map(lambda (hash,feats,label):(hash,(feats,label)))

    #merge train bytes and train asm data
    train = train_bytes.join(train_asm).map(lambda (hash,((bytes,label),(asm,label2))):(hash,bytes,asm,label))
    schema = StructType([StructField('hash',StringType(),True),StructField('bytes',VectorUDT(),True),StructField('asm',VectorUDT(),True),StructField('label',StringType(),True)])
    train = train.toDF(schema)
    train = train.withColumn('label',train.label.cast(DoubleType()))
    print "Final TrainDF done!!!!!!!"
   
    '''
    we want to use the same pipeline when the testing labels are present or absent, so, the below code builds 
    the test data accordingly.
    The test RDD looks like:
        - When test labels are present: <hash,selectedFeatures,label>
        - When test labels are absent : <hash,selectedFeatures>
    
    When labels are absent, there is no 3rd column in dataframe, and so, the consecutive dataframes also
    don't have that column.
    '''
    if y_test is not None:
        test_bytes = test_bytes.select('hash','selectedFeatures','label').rdd.map(lambda (hash,feats,label):(hash,(feats,label)))
        test_asm = test_asm.select('hash','selectedFeatures','label').rdd.map(lambda (hash,feats,label):(hash,(feats,label)))

        #merge test bytes and test asm data
        test = test_bytes.join(test_asm).map(lambda (hash,((bytes,label),(asm,label2))):(hash,bytes,asm,label))
        schema = StructType([StructField('hash',StringType(),True),StructField('bytes',VectorUDT(),True),StructField('asm',VectorUDT(),True),StructField('label',StringType(),True)])
        test = test.toDF(schema)
        test = test.withColumn('label',test.label.cast(DoubleType()))
      
    else:
        test_bytes = test_bytes.select('hash','selectedFeatures').rdd.map(lambda (hash,feats):(hash,feats))    
        test_asm = test_asm.select('hash','selectedFeatures').rdd.map(lambda (hash,feats):(hash,feats))

        #merge test bytes and test asm data
        test = test_bytes.join(test_asm).map(lambda (hash,(bytes,asm)):(hash,bytes,asm))
        schema = StructType([StructField('hash',StringType(),True),StructField('bytes',VectorUDT(),True),StructField('asm',VectorUDT(),True)])
        test = test.toDF(schema)
        
    print "Final TestDF done!!!!!!!"    
        
    #merge bytes and asm features
    assembler = VectorAssembler(inputCols=["bytes", "asm"],outputCol="features")
    train = assembler.transform(train)
    test = assembler.transform(test)
    print "VectorAssembler done!!!!!!!"    
    

    #rf classifier
    rf = RandomForestClassifier(numTrees=100,maxDepth=8,maxBins=48,maxMemoryInMB=512,seed=1)
    model = rf.fit(train)
    result = model.transform(test)

    #save to csv
    #result.select("prediction").toPandas().astype(int).to_csv('prediction.csv',header=False,index=False)
    hash_predictions = result.select("hash","prediction")
    result.select("hash","prediction").toPandas().to_csv('./prediction.csv',header=False,index=False)


    #Get all the predictions in the same order as hashes in X_test and save them to a file
    test_hashes = X_test.zipWithIndex().map(lambda x:(x[1],x[0].encode('utf-8')))
    predictions = hash_predictions.collect()
    predictionsBroadCast = sc.broadcast(dict(predictions))
    predicted_labels = test_hashes.map(lambda (indx,docHash):(docHash, get_predicted_label(docHash,predictionsBroadCast.value))).map(lambda (docHash,label):int(label)).collect()
    save_predicted_labels(predicted_labels,'./predictions.txt')
   

    spark.stop()

if __name__ == "__main__":
    main()    

