from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import *
from pyspark.rdd import RDD
from pyspark.ml.linalg import SparseVector, VectorUDT
import numpy as np

class preprocessor(object):
    def __init__(self):
        #load list of most common bigrams from training set
        bigrams = list(np.load('./data/most_common_bigrams.npy'))
        bigrams = dict(enumerate(bigrams))
        self.top_bigrams = {y:x for x,y in bigrams.iteritems()}
    
    def _term_frequency(self,path):
        '''
        get term frequency of 4-hexadecimal-character words
        '''
        #get byte file from hash and tokenize
                
        #add bigrams to sparse dic
        sparse_dic = {}
        for idx in xrange(len(tokens)-1):
            word = tokens[idx]+tokens[idx+1]
            converted = int(word,16)
            if converted in self.top_bigrams:
                key = self.top_bigrams[converted]
                if key in sparse_dic:
                    sparse_dic[key] += 1.
                else:
                    sparse_dic[key] = 1.

        tf = SparseVector(len(self.top_bigrams),sparse_dic)
        return tf

    def transform(self,X_rdd,y_rdd=None):
        '''
        given X RDD (and optionally y RDD), output dataframe with term frequency feature vector and labels
        '''    
        #check input type
        if type(X_rdd) != RDD:
            raise TypeError("Arguments must be pySpark RDDs")
        if y_rdd and type(y_rdd) != RDD:
            raise TypeError("Arguments must be pySpark RDDs")
        
        #convert X to URL paths
        X = X_rdd.map(lambda x: 's3n://eds-uga-csci8360/data/project2/' + x.replace('\n','') + '.bytes')
        X = X.map(self._term_frequency)
        
        #check if labels exist
        if y_rdd:
            #combine X and y into single dataframe
            X = X.zipWithIndex().map(lambda r: (r[1],r[0]))
            y = y_rdd.zipWithIndex().map(lambda r: (r[1],r[0]))
            data = X.join(y).map(lambda r: r[1])
            schema = StructType([StructField('features',VectorUDT(),True),StructField('label',StringType(),True)])
            data = data.toDF(schema)
            data = data.withColumn('label',data.label.cast(DoubleType()))
        
        else:
            X = X.map(lambda row: [row])
            schema = StructType([StructField("features", VectorUDT(), True)])
            data = X.toDF(schema)
            
        return data

if __name__ == '__main__':

    #initialize spark session
    spark = SparkSession\
            .builder\
            .appName("Test")\
            .getOrCreate()
    sc = spark.sparkContext
    
    aws_id = 'AKIAJNFUBSR36COMCZMA'
    aws_key = 'C6L6gaWf1DY+SLaalmkOA/Uzf7Ybd8N27Y2M7dJG'
    
    sc._jsc.hadoopConfiguration().set("fs.s3n.awsAccessKeyId", aws_id)
    sc._jsc.hadoopConfiguration().set("fs.s3n.awsSecretAccessKey", aws_key)

    bigrams = list(np.load('./data/most_common_bigrams.npy'))
    bigrams = dict(enumerate(bigrams))
    top_bigrams = {y:x for x,y in bigrams.iteritems()}
    
    #paths to training data
    X_file = "./data/X_train_small.txt"
    y_file = "./data/y_train_small.txt"
    
    y_file = sc.textFile(y_file)
       
    with open(X_file,'r') as f:
        binaries = f.readlines()
        
    binaries = ['s3n://eds-uga-csci8360/data/project2/binaries/%s.bytes' % x.rstrip() for x in binaries]
    rdds = []
    
    for i in range(len(binaries)):
        sparse_dic = sc.textFile(binaries[i])\
            .map(lambda line: [str(word) for word in line.split(' ') if len(word)==2 and word!="??"])\
            .map(lambda line: zip(line,line[1:]))\
            .map(lambda bigrams: [int(pair[0]+pair[1],16) for pair in bigrams if int(pair[0]+pair[1],16) in top_bigrams])\
            .flatMap(lambda key: key)\
            .map(lambda key: (top_bigrams[key],1))\
            .reduceByKey(lambda acc, w: acc + w).collectAsMap()
        tf = SparseVector(len(top_bigrams),sparse_dic)
        rdds.append(sc.parallelize([i,tf]))
    
    X = sc.union(rdds)
    y = y_file.zipWithIndex().map(lambda r: (r[1],r[0]))
    data = X.join(y).map(lambda r: r[1])
    schema = StructType([StructField('features',VectorUDT(),True),StructField('label',StringType(),True)])
    data = data.toDF(schema)
    data = data.withColumn('label',data.label.cast(DoubleType()))
    
    print data.show()
    
    try:
        data.write.save("./data/train_small.parquet")
    except:
        pass
    
    '''
    #preprocess data
    preprocessor1 = preprocessor()
    data = preprocessor1.transform(X_file,y_file)
    
    print data.show()
    
    #save to parquet
    try:
        data.write.save("./data/train_small.parquet")
    except:
        pass
        
    #paths to test data
    X_file = "./data/X_test_small.txt"
    y_file = "./data/y_test_small.txt"
    X_file = sc.textFile(X_file)
    y_file = sc.textFile(y_file)
    
    #preprocess data
    preprocessor2 = preprocessor()
    data = preprocessor2.transform(X_file,y_file)
    
    print data.show()
    
    #save to parquet
    try:
        data.write.save("./data/test_small.parquet")
    except:
        pass
    '''