from pyspark.mllib.util import MLUtils
from pyspark.sql import SparkSession
import os
import shutil as sh
from collections import Counter
from pyspark.rdd import RDD
import numpy as np
from itertools import chain
from pyspark.sql import functions as sql_functions
import sys
from pyspark.sql.types import *
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.linalg import SparseVector,Vectors, VectorUDT
from pyspark.ml.evaluation import MulticlassClassificationEvaluator


class preprocessor(object):

    def __init__(self):
        """
        On initialization, reads the list of bigrams
        """
        bigrams = list(np.load('./data/most_common_bigrams.npy'))
        bigrams = dict(enumerate(bigrams))
        self.top_bigrams = dict((y,x) for x,y in bigrams.iteritems())
    
    def clean_doc(self,docURL,docContents):
        """
        1. Cleans the hashes and words from X_train and binaries respectively
        2. Creates a flatMap of all the words corresponding to every hash
        3. Returns: (document_hash,all_words_in_that_hash)
        """    
        docHash = docURL.encode('utf-8').rsplit('/',1)[1].split('.bytes')[0]
        docLines = docContents.split("\r")
        words = []
        for line in docLines:
            separatedLine = line.encode('utf-8').strip().split(' ')[1:]
            words.append([word for word in separatedLine if len(word)==2 and word!='??'])
            
        allWords = list(chain.from_iterable(words))    
        return (docHash,allWords)

    def checkInTopBigram(self,pair):
        """
        Checks whether a bigram is one of the top bigrams
        """    
        x = int(pair[0]+pair[1],16)
        if x in self.top_bigrams:
            return x

    def get_bigram(self,key):
        """
        Returns the bigram associated with a index
        """    
        if key!=None:
            return self.top_bigrams[key]

    def extract_frequent_bigrams(self,docContents):
        """
        1. Out of all the words of the a binary, the below function only retains the frequent bigrams
        2. Returns : SparseVector(bigrams)
        """    
        bigrams = zip(docContents,docContents[1:])
        bigramKeys = map(lambda pair: self.checkInTopBigram(pair), bigrams)
        cleanedBigrams = filter(lambda x: x  and x!=None, bigramKeys)
        bigramKeysFlattened = map(lambda key: self.get_bigram(key), cleanedBigrams)
        sparse_dic = dict(Counter(bigramKeysFlattened))
        tf = SparseVector(len(self.top_bigrams),sparse_dic)
        return tf

    def get_hashes_and_labels(self,X_rdd,y_rdd):
        """
        Returns: RDD(hash,label)
        """    
        #Check input type
        if type(X_rdd) != RDD:
            raise TypeError("Arguments must be pySpark RDDs")
        if y_rdd and type(y_rdd) != RDD:
            raise TypeError("Arguments must be pySpark RDDs")

        hashes = X_rdd.map(lambda hash:hash.encode("utf-8")).zipWithIndex().map(lambda x:(x[1],x[0])) 
        labels = y_rdd.map(lambda hash:hash.encode("utf-8")).zipWithIndex().map(lambda x:(x[1],x[0]))   
       
        #Join X_rdd(hashes) and y_rdd(labels)
        hash_and_labels = hashes.join(labels).map(lambda (x,(y1,y2)):(y1,y2)) 
        return hash_and_labels 

    def get_binary_names(self,hash_and_labels_rdd) :
        """
        Given X RDD, which has the hashes, return a list of names of the binaries
        """    
        binary_file_names = hash_and_labels_rdd.map(lambda doc: 's3n://eds-uga-csci8360/data/project2/binaries/'+doc[0]+'.bytes').collect()
        return list(binary_file_names)

    def transform(self,hash_and_labels_rdd,binaries_rdd):
        #clean all lines and create: (docHash, docWords)
        cleaned_binary_files = binaries_rdd.map(lambda doc:self.clean_doc(doc[0],doc[1]))
        print "Cleaned binaryFiles"

        #find the bigrams
        bigrammed_binaries = cleaned_binary_files.map(lambda doc:(doc[0],self.extract_frequent_bigrams(doc[1])))
        print "Bigrammed binaryFiles"

        #Put everything together: (hash,label,docwWords)
        joined_rdd = hash_and_labels_rdd.join(bigrammed_binaries)
        print "Joined hash, labels, bigrams"

        return joined_rdd

            
def main():
        
    if len(sys.argv) < 4:
        sys.exit('Please specify all the file names in the following order: X_train, y_train, X_test, y_test')

#-------------------------------------------------------------------------------
# Initialize spark session
#-------------------------------------------------------------------------------
    spark = SparkSession\
            .builder\
            .appName("Test")\
            .getOrCreate()
    sc = spark.sparkContext

    sc._jsc.hadoopConfiguration().set("fs.s3n.awsAccessKeyId", "***REMOVED***")
    sc._jsc.hadoopConfiguration().set("fs.s3n.awsSecretAccessKey", "***REMOVED***")

#-------------------------------------------------------------------------------
# Read the filenames and initialize preprocessor
#-------------------------------------------------------------------------------
    fileNames = (sys.argv)
    X_train_file = fileNames[1]
    y_train_file = fileNames[2]
    X_test_file = fileNames[3]
    #y_test_file = fileNames[4]
    
    
#-------------------------------------------------------------------------------
# Read the training: hashes, labels and binaries
#-------------------------------------------------------------------------------
    #1. Initialize preprocessor 
    
    preprocessor1 = preprocessor()
    hash_values = sc.textFile(X_train_file,25)
    label_values = sc.textFile(y_train_file,25)

    #2. Put (hashes,labels) together
    hash_and_label = preprocessor1.get_hashes_and_labels(hash_values,label_values)
    print hash_and_label.take(1)
    
    #3. Read the binaries
    training_binaries_fileNames = preprocessor1.get_binary_names(hash_and_label)
    training_binaries_fileNames_bc = sc.broadcast(list(training_binaries_fileNames))
    

    training_binaries = sc.wholeTextFiles(','.join(training_binaries_fileNames_bc.value),25)
    #training_binaries = sc.wholeTextFiles('./binaries/*',25)
    hash_labels_bigrams = preprocessor1.transform(hash_and_label,training_binaries) 
    print hash_labels_bigrams.take(1)
    
    
    #4. Convert to dataframe
    training_features_label_df = hash_labels_bigrams.map(lambda (hash,(label,words)): (words,label))
    schema = StructType([StructField('features',VectorUDT(),True),StructField('label',StringType(),True)])   
    training_features_label_df = training_features_label_df.toDF(schema)
    print "Created the training dataframe!!"
    
    
#-------------------------------------------------------------------------------
# Read the Testing Data: hashes, labels and binaries
#-------------------------------------------------------------------------------
    #1. Initialize preprocessor 
    preprocessor2 = preprocessor()
    test_hash_values = sc.textFile(X_test_file,25)
    
    #2. Put (hashes,labels) together
    test_hash_and_label = test_hash_values.map(lambda hash:hash.encode("utf-8")).zipWithIndex()
    
    #3. Read the binaries
    testing_binaries_fileNames = preprocessor2.get_binary_names(test_hash_and_label)
    training_binaries_fileNames_bc = sc.broadcast(list(testing_binaries_fileNames))

    testing_binaries = sc.wholeTextFiles(','.join(training_binaries_fileNames_bc.value),25)
    test_hash_labels_bigrams = preprocessor2.transform(test_hash_and_label,testing_binaries) 

    #4. Convert to dataframe
    testing_features_label_df = test_hash_labels_bigrams.map(lambda (hash,(label,words)): (words,label))
    schema = StructType([StructField('features',VectorUDT(),True),StructField('label',StringType(),True)])   
    testing_features_label_df = testing_features_label_df.toDF(schema)
    print "Created the testing dataframe!!"

#-------------------------------------------------------------------------------
# Read the training data and build the model
#-------------------------------------------------------------------------------
    #load data
    train = training_features_label_df.withColumn('label',training_features_label_df.label.cast(DoubleType()))
    train = train.withColumn('features',train.features.cast(VectorUDT()))
    print "training dataframe read!!"

    rf = RandomForestClassifier(labelCol="label", featuresCol="features", numTrees=150,impurity='gini',maxDepth=8, maxBins=32,seed=42) 
    model = rf.fit(train)
    print "Model built!!"

#-------------------------------------------------------------------------------
# Read the testing data and predict
#-------------------------------------------------------------------------------
    #load data
    test = testing_features_label_df.withColumn('label',testing_features_label_df.label.cast(DoubleType()))
    test = test.withColumn('features',test.features.cast(VectorUDT()))
    print "testing dataframe read!!"

    result = model.transform(test)

    #save to csv
    result.select("prediction").toPandas().to_csv('/home/ec2-user/prediction.csv',header=False,index=False)
    print "results written"

    #save to csv
    result.select("label").toPandas().to_csv('/home/ec2-user/prediction.csv',header=False,index=False)
    print "label written"

    x = result.select("prediction").collect()
    orig_stdout = sys.stdout
    f = file('/home/ec2-user/predictions.txt', 'w')
    sys.stdout = f

    for i in x:
        print i

    sys.stdout = orig_stdout
    f.close() 

    print "results after collect"

    spark.stop()


if __name__ == "__main__":
    main()    
