from pyspark.sql import SparkSession
from collections import Counter
from pyspark.sql.functions import udf
from pyspark.sql.types import *
from pyspark.rdd import RDD
from pyspark.ml.linalg import SparseVector, VectorUDT
import numpy as np
import urllib
from itertools import chain
from pyspark.sql import Row
from pyspark.ml.feature import NGram
import re
from pyspark.sql import functions as sql_functions


def clean_doc(docURL,docContents):
    docHash = docURL.encode('utf-8').rsplit('/',1)[1].split('.bytes')[0]
    docLines = docContents.split("\r")
    words = []
    for line in docLines:
        separatedLine = line.encode('utf-8').strip().split(' ')[1:]
        words.append([word for word in separatedLine if len(word)==2 and word!='??'])
        
    allWords = list(chain.from_iterable(words))    
    return (docHash,allWords)

def find_bigrams(docHash,docContents):
    bigrams = zip(docContents,docContents[1:])
    index = map(lambda pair: int(pair[0]+pair[1],16), bigrams)
    return (docHash,dict(Counter(index)))


            
if __name__ == '__main__':

#-------------------------------------------------------------------------------
# Initialize spark session
#-------------------------------------------------------------------------------
    spark = SparkSession\
            .builder\
            .appName("Test")\
            .getOrCreate()
    sc = spark.sparkContext

#-------------------------------------------------------------------------------
# Read the Training Data: Hashes and Labels
#-------------------------------------------------------------------------------
    #paths to training data
    X_file = "./data/X_train_small.txt"
    y_file = "./data/y_train_small.txt"
    
    #read all the hashes
    hashValues = sc.textFile(X_file,50).map(lambda hash:hash.encode("utf-8")).zipWithIndex().map(lambda x:(x[1],x[0]))
    print "Read hashValues"
    
    #get all the file names
    binaryTrainingFileNames = hashValues.map(lambda doc: 's3://eds-uga-csci8360/data/project2/binaries/'+doc[1]+'.bytes').toLocalIterator()
   
    #read all the labels
    labelValues = sc.textFile(y_file,50).map(lambda hash:hash.encode("utf-8")).zipWithIndex().map(lambda x:(x[1],x[0]))
    print "Read labelValues"
    #labelValuesBroadCast = sc.broadcast(labelValues.collectAsMap())

    #join labels and hashes
    hashAndLabel = hashValues.join(labelValues).map(lambda (x,(y1,y2)):(y1,y2)) 
    print "Joined hash and labels"
   
#-------------------------------------------------------------------------------
# Read the Training Data: Binaries
#-------------------------------------------------------------------------------
    
    #Read the binaries for now:
    binaryFiles = sc.wholeTextFiles(','.join(list(binaryTrainingFileNames)),50)
    print "Read binaryFiles,count of files"
    print binaryFiles.count()

    #clean all lines and create: (docHash, docWords)
    cleanBinaryFiles = binaryFiles.map(lambda doc:clean_doc(doc[0],doc[1]))
    print "Cleaned binaryFiles"

    #find the bigrams
    bigrammedBinaryFiles = cleanBinaryFiles.map(lambda doc:find_bigrams(doc[0],doc[1]))
    print "Bigrammed binaryFiles"
    
    #Put everything together: hash,docWords,label:
    joinedRDD = hashAndLabel.join(bigrammedBinaryFiles)
    print "Joined hash, bigrams, labels"

    #convert every row to LabeledPoint
    transformedJoinedRDD = joinedRDD.map(lambda (hash,(label,words)): LabeledPoint(int(label)-1,words))
    #print transformedTrainingRDD.show()
    
    #Save the RDD in LibSVM format, as Naive Bayes reads in the same format
    MLUtils.saveAsLibSVMFile(transformedJoinedRDD,"trainingLibsvmfile")
    training = MLUtils.loadLibSVMFile(sc, "trainingLibsvmfile/*")
    print "trainingLibsvmfile created!!"
    

#-------------------------------------------------------------------------------
# Read the Testing Data: Hashes, Labels
#-------------------------------------------------------------------------------
    
    #paths to testing data
    X_test_file = "./data/X_test_small.txt"
    y_test_file = "./data/y_test_small.txt"

    #read all test hashes
    testhashValues = sc.textFile(X_test_file,50).map(lambda hash:hash.encode("utf-8")).zipWithIndex().map(lambda x:(x[1],x[0]))
    print "Read test hashValues"

    #get all the file names
    binaryTestingFileNames = testhashValues.map(lambda doc: 's3://eds-uga-csci8360/data/project2/binaries/'+doc[1]+'.bytes').toLocalIterator()


    #read all test labels
    testlabelValues = sc.textFile(y_test_file,50).map(lambda hash:hash.encode("utf-8")).zipWithIndex().map(lambda x:(x[1],x[0]))
    print "Read test labelValues"
    #labelValuesBroadCast = sc.broadcast(labelValues.collectAsMap())

    #join test labels and hashes
    testhashAndLabel = testhashValues.join(testlabelValues).map(lambda (x,(y1,y2)):(y1,y2)) 
    print "Joined test hash and labels"
    

#-------------------------------------------------------------------------------
# Read the Testing Data: Binaries
#-------------------------------------------------------------------------------
    
    #Read the test binaries for now:
    testbinaryFiles = sc.wholeTextFiles(','.join(list(binaryTestingFileNames)),50)
    print "Read test binaryFiles"

    #clean all lines and create: (docHash, docWords)
    testcleanBinaryFiles = testbinaryFiles.map(lambda doc:clean_doc(doc[0],doc[1]))
    print "Cleaned test binaryFiles"

    #find the bigrams
    testbigrammedBinaryFiles = testcleanBinaryFiles.map(lambda doc:find_bigrams(doc[0],doc[1]))
    print "Bigrammed test binaryFiles"
    
    #Put everything together: hash,docWords,label:
    testjoinedRDD = testhashAndLabel.join(testbigrammedBinaryFiles)
    print "Joined test hash, bigrams, labels"

    #convert every row to LabeledPoint
    transformedTestRDD =  testjoinedRDD.map(lambda (hash,(label,words)): LabeledPoint(int(label)-1,words))

    MLUtils.saveAsLibSVMFile(transformedTestRDD,"testingLibsvmfile")
    testingData = MLUtils.loadLibSVMFile(sc, "testingLibsvmfile/*")
    print "testingLibsvmfile created!!"
    


