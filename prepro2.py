from pyspark.mllib.tree import RandomForest, RandomForestModel
from pyspark.mllib.util import MLUtils
from pyspark.mllib.linalg import Vectors
from pyspark.mllib.linalg import SparseVector
from pyspark.mllib.regression import LabeledPoint
from pyspark.sql import SparkSession
import os
import shutil as sh
from pyspark.sql import SparkSession
from collections import Counter
from pyspark.sql.functions import udf
from pyspark.sql.types import *
from pyspark.rdd import RDD
import numpy as np
import urllib
from itertools import chain
from pyspark.sql import Row
import re
from pyspark.sql import functions as sql_functions
import sys


def get_top_bigrams():
    #load list of most common bigrams from training set
    bigrams = list(np.load('./data/most_common_bigrams.npy'))
    bigrams = dict(enumerate(bigrams))
    top_bigrams = dict((y,x) for x,y in bigrams.iteritems())
    return top_bigrams

def clean_doc(docURL,docContents):
    docHash = docURL.encode('utf-8').rsplit('/',1)[1].split('.bytes')[0]
    docLines = docContents.split("\r")
    words = []
    for line in docLines:
        separatedLine = line.encode('utf-8').strip().split(' ')[1:]
        words.append([word for word in separatedLine if len(word)==2 and word!='??'])
            
    allWords = list(chain.from_iterable(words))    
    return (docHash,allWords)

def checkInTopBigram(pair):
    x = int(pair[0]+pair[1],16)
    if x in top_bigrams:
        return x


def get_bigram(key):
    if key!=None:
        return top_bigrams[key]

def find_frequent_bigrams(docContents,top_bigrams):
    bigrams = zip(docContents,docContents[1:])
    bigramKeys = map(lambda pair: checkInTopBigram(pair), bigrams)
    cleanedBigrams = filter(lambda x: x  and x!=None, bigramKeys)
    bigramKeysFlattened = map(lambda key: get_bigram(key), cleanedBigrams)
    sparse_dic = dict(Counter(bigramKeysFlattened))
    tf = SparseVector(len(top_bigrams),sparse_dic)
    return tf


            
if __name__ == '__main__':

    if len(sys.argv) < 5:
        sys.exit('Please specify all the file names in the following order: X_train, y_train, X_test, y_test')

#-------------------------------------------------------------------------------
# Initialize spark session
#-------------------------------------------------------------------------------
    spark = SparkSession\
            .builder\
            .appName("Test")\
            .getOrCreate()
    sc = spark.sparkContext

    #get all the top bigrams
    top_bigrams = get_top_bigrams()

#-------------------------------------------------------------------------------
# Read the Training Data: Hashes and Labels
#-------------------------------------------------------------------------------
    #paths to training data
    #X_file = "s3n://eds-uga-csci8360/data/project2/labels/X_train_small.txt"
    #y_file = "s3n://eds-uga-csci8360/data/project2/labels/y_train_small.txt"
    
    #paths to testing data
    #X_test_file = "s3n://eds-uga-csci8360/data/project2/labels/X_test_small.txt"
    #y_test_file = "s3n://eds-uga-csci8360/data/project2/labels/y_test_small.txt"

    fileNames = (sys.argv)
    X_train_file = fileNames[1]
    y_train_file = fileNames[2]
    X_test_file = fileNames[3]
    y_test_file = fileNames[4]

    #read all the hashes
    hashValues = sc.textFile(X_train_file,15).map(lambda hash:hash.encode("utf-8")).zipWithIndex().map(lambda x:(x[1],x[0]))
    print "Read hashValues"
    
    #get all the file names
    binaryTrainingFileNames = hashValues.map(lambda doc: 's3n://eds-uga-csci8360/data/project2/binaries/'+doc[1]+'.bytes').toLocalIterator()
    binaryTrainingFileNamesBroadCast = sc.broadcast(list(binaryTrainingFileNames))

    #read all the labels
    labelValues = sc.textFile(y_train_file,15).map(lambda hash:hash.encode("utf-8")).zipWithIndex().map(lambda x:(x[1],x[0]))
    print "Read labelValues"
    #labelValuesBroadCast = sc.broadcast(labelValues.collectAsMap())

    #join labels and hashes
    hashAndLabel = hashValues.join(labelValues).map(lambda (x,(y1,y2)):(y1,y2)) 
    print "Joined hash and labels"
   

#-------------------------------------------------------------------------------
# Read the Training Data: Binaries
#-------------------------------------------------------------------------------
    
    #Read the binaries for now:
    binaryFiles = sc.wholeTextFiles(','.join(binaryTrainingFileNamesBroadCast.value),15)
    print "Read binaryFiles,count of files"
    print binaryFiles.count()

    #clean all lines and create: (docHash, docWords)
    cleanBinaryFiles = binaryFiles.map(lambda doc:clean_doc(doc[0],doc[1]))
    print "Cleaned binaryFiles"

    #find the bigrams
    bigrammedBinaryFiles = cleanBinaryFiles.map(lambda doc:(doc[0],find_frequent_bigrams(doc[1],top_bigrams)))
    print "Bigrammed binaryFiles"
    #print bigrammedBinaryFiles.take(1)
    
    #Put everything together: hash,docWords,label:
    joinedRDD = hashAndLabel.join(bigrammedBinaryFiles)
    print "Joined hash, bigrams, labels"

    #convert every row to LabeledPoint
    transformedJoinedRDD = joinedRDD.map(lambda (hash,(label,words)): LabeledPoint(int(label)-1,words))
    #print transformedTrainingRDD.show()
    print "Converted to LabeledPoint"
    
    #Save the RDD in LibSVM format, as Naive Bayes reads in the same format
    MLUtils.saveAsLibSVMFile(transformedJoinedRDD,"./libsvms/trainingLibsvmfile")
    #training = MLUtils.loadLibSVMFile(sc, "trainingLibsvmfile/*")
    print "trainingLibsvmfile created!!"
    
    
#-------------------------------------------------------------------------------
# Read the Testing Data: Hashes, Labels
#-------------------------------------------------------------------------------
    #read all test hashes
    testhashValues = sc.textFile(X_test_file,15).map(lambda hash:hash.encode("utf-8")).zipWithIndex().map(lambda x:(x[1],x[0]))
    print "Read test hashValues"

    #get all the file names
    binaryTestingFileNames = testhashValues.map(lambda doc: 's3n://eds-uga-csci8360/data/project2/binaries/'+doc[1]+'.bytes').toLocalIterator()
    binaryTestingFileNamesBroadCast = sc.broadcast(list(binaryTestingFileNames))

    #read all test labels
    testlabelValues = sc.textFile(y_test_file,15).map(lambda hash:hash.encode("utf-8")).zipWithIndex().map(lambda x:(x[1],x[0]))
    print "Read test labelValues"
    #labelValuesBroadCast = sc.broadcast(labelValues.collectAsMap())

    #join test labels and hashes
    testhashAndLabel = testhashValues.join(testlabelValues).map(lambda (x,(y1,y2)):(y1,y2)) 
    print "Joined test hash and labels"
    

#-------------------------------------------------------------------------------
# Read the Testing Data: Binaries
#-------------------------------------------------------------------------------
    
    #Read the test binaries for now:
    testbinaryFiles = sc.wholeTextFiles(','.join(binaryTestingFileNamesBroadCast.value),15)
    print "Read test binaryFiles"

    #clean all lines and create: (docHash, docWords)
    testcleanBinaryFiles = testbinaryFiles.map(lambda doc:clean_doc(doc[0],doc[1]))
    print "Cleaned test binaryFiles"

    #find the bigrams
    testbigrammedBinaryFiles = testcleanBinaryFiles.map(lambda doc:(doc[0],find_frequent_bigrams(doc[1],top_bigrams)))
    print testbigrammedBinaryFiles.take(1)
    print "Bigrammed test binaryFiles"
    
    #Put everything together: hash,docWords,label:
    testjoinedRDD = testhashAndLabel.join(testbigrammedBinaryFiles)
    print "Joined test hash, bigrams, labels"

    #convert every row to LabeledPoint
    transformedTestRDD =  testjoinedRDD.map(lambda (hash,(label,words)): LabeledPoint(int(label)-1,words))
    print "Converted to LabeledPoint"

    MLUtils.saveAsLibSVMFile(transformedTestRDD,"./libsvms/testingLibsvmfile")
    #testingData = MLUtils.loadLibSVMFile(sc, "testingLibsvmfile/*")
    print "testingLibsvmfile created!!"
    
